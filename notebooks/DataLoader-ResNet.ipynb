{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset # wraps an iterable around the dataset\n",
    "from torchvision import datasets    # stores the samples and their corresponding labels\n",
    "from torchvision.transforms import transforms  # transformations we can perform on our dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# q: what is the difference between torch.nn.functional and torch.nn\n",
    "# a: https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"cf61e02cee13abdd3d8a232d29df527bd6cc7f89\"\n",
    "\n",
    "# Set the WANDB_NOTEBOOK_NAME environment variable to the name of your notebook (manually)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"DataLoader.ipynb\"\n",
    "\n",
    "# set the WANDB_TEMP environment variable to a directory where we have write permissions\n",
    "os.environ[\"WANDB_TEMP\"] = os.getcwd()\n",
    "os.environ[\"WANDB_DIR\"] = os.getcwd()\n",
    "os.environ[\"WANDB_CONFIG_DIR\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mECG-analysis-with-Deep-Learning-on-GPU-accelerators\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "wandb.init(project='ECG-analysis-with-Deep-Learning-on-GPU-accelerators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training \n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, split='train'):\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        # data loading\n",
    "        current_directory = os.getcwd()\n",
    "        self.parent_directory = os.path.dirname(current_directory)\n",
    "        train_small_path = os.path.join(self.parent_directory, 'data/data', 'deepfake-ecg-small', str(self.split) + '.csv')\n",
    "        self.df = pd.read_csv(train_small_path)  # Skip the header row\n",
    "        \n",
    "        # Avg RR interval\n",
    "        # in milli seconds\n",
    "        RR = torch.tensor(self.df['pr'].values, dtype=torch.float32)\n",
    "        # calculate HR\n",
    "        self.y = 60 * 1000/RR\n",
    "\n",
    "        # Size of the dataset\n",
    "        self.samples = self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # file path\n",
    "        filename= self.df['patid'].values[index]\n",
    "        asc_path = os.path.join(self.parent_directory, 'data/data', 'deepfake-ecg-small', str(self.split), str(filename) + '.asc')\n",
    "        #print(asc_path)\n",
    "        \n",
    "        ecg_signals = pd.read_csv( asc_path, header=None, sep=\" \") # read into dataframe\n",
    "        ecg_signals = torch.tensor(ecg_signals.values) # convert dataframe values to tensor\n",
    "        \n",
    "        ecg_signals = ecg_signals.float()\n",
    "        \n",
    "        # Transposing the ecg signals\n",
    "        ecg_signals = ecg_signals/6000 # normalization\n",
    "        ecg_signals = ecg_signals.t() \n",
    "        \n",
    "        qt = self.y[index]\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return ecg_signals, qt\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG dataset\n",
    "train_dataset = ECGDataSet(split='train')\n",
    "validate_dataset = ECGDataSet(split='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first data\n",
    "first_data = train_dataset[2]\n",
    "x, y = first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0123, -0.0147, -0.0088,  ..., -0.0090, -0.0063, -0.0090],\n",
       "        [-0.0053, -0.0075, -0.0115,  ..., -0.0048, -0.0043, -0.0097],\n",
       "        [-0.0030,  0.0000, -0.0017,  ...,  0.0068,  0.0083,  0.0085],\n",
       "        ...,\n",
       "        [-0.0110, -0.0078, -0.0065,  ..., -0.0052, -0.0030, -0.0037],\n",
       "        [-0.0127, -0.0162, -0.0115,  ..., -0.0115, -0.0108, -0.0167],\n",
       "        [-0.0055, -0.0072, -0.0018,  ..., -0.0060, -0.0045, -0.0068]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(389.6104)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5000])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Residual Convoluted Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# It allows you to efficiently load and iterate over batches of data during the training or evaluation process.\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, num_workers=20)\n",
    "validate_dataloader = DataLoader(dataset=validate_dataset, batch_size=8, shuffle=False, num_workers=20)\n",
    "\n",
    "# q: what is num_workers?\n",
    "# A: num_workers (int, optional) â€“ how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 5000]) torch.Size([4])\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.dtype, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet of the paper reimplementation with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanResWide_X(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        super(KanResWide_X, self).__init__()\n",
    "        #q: what does super(KanResWide_X, self) do?\n",
    "        #a: it returns a proxy object that delegates method calls to a parent or sibling class of type.\n",
    "        #q: what does super(KanResWide_X, self).__init__() do?\n",
    "        #a: it calls the __init__ function of the parent class (nn.Module)\n",
    "\n",
    "        #q: is super(KanResWide_X, self).__init__() same to super().__init__()?\n",
    "        #a: yes, but the former is more explicit\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # initial module (before resnet blocks)\n",
    "        self.kanres_init = nn.Sequential(\n",
    "            nn.Conv1d(input_size, 64, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 32, kernel_size=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Resnet block\n",
    "        self.kanres_module = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=50, stride=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 32, kernel_size=50, stride=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.functional.add()        # the skip connection in res block\n",
    "            #q: what does nn.Add() do?\n",
    "            #a: it adds the input to the output\n",
    "        )\n",
    "\n",
    "        self.global_average_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dense = nn.Linear(32, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.kanres_init(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.global_average_pooling(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class KanResInit(nn.Module):\n",
    "    def __init__(self, in_channels, filterno_1, filterno_2, filtersize_1, filtersize_2, stride):\n",
    "        #print(in_channels) --> 8\n",
    "        super(KanResInit, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, filterno_1, filtersize_1, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(filterno_1)\n",
    "        self.conv2 = nn.Conv1d(filterno_1, filterno_2, filtersize_2)\n",
    "        self.bn2 = nn.BatchNorm1d(filterno_2)\n",
    "        # initialize a relu layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class KanResModule(nn.Module):\n",
    "    def __init__(self, in_channels, filterno_1, filterno_2, filtersize_1, filtersize_2, stride):\n",
    "        super(KanResModule, self).__init__()\n",
    "        # have to use same padding to keep the size of the input and output the same\n",
    "        # calculate the padding needed for same\n",
    "        padding = (filtersize_1 - 1) // 2 + (stride - 1)\n",
    "        self.conv1 = nn.Conv1d(in_channels, filterno_1, filtersize_1, stride=stride, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(filterno_1)\n",
    "        self.conv2 = nn.Conv1d(filterno_1, filterno_2, filtersize_2, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(filterno_2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        #print(x.shape)      \n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x + identity\n",
    "        return x\n",
    "\n",
    "class KanResWide_X2(nn.Module):\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(KanResWide_X2, self).__init__()\n",
    "\n",
    "        #print(input_shape[0])\n",
    "        #print(input_shape[1])\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.init_block = KanResInit(input_shape[0], 64, 32, 8, 3, 1)\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "        self.module_blocks = nn.Sequential(\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.init_block(x)\n",
    "        #print(\"init block trained\")\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pool 1 trained\")\n",
    "        #print(x.shape)\n",
    "        x = self.module_blocks(x)\n",
    "        #print(\"module blocks trained\")\n",
    "        x = self.global_avg_pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #q: explain the above line\n",
    "        #a: it flattens the input\n",
    "        x = self.fc(x)\n",
    "        #print(x.shape)\n",
    "        # squeeze the output\n",
    "        x = torch.squeeze(x)\n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step reimplementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small 1D CNN just to check if the model is working\n",
    "class CNNblock(nn.Module):\n",
    "    def __init__(self,input_channels):\n",
    "        super(CNNblock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # q: explain nn.Conv1d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # a: input_channels is the number of channels in the input data\n",
    "        #    32 is the number of output channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        # q: what is 32?\n",
    "        # a: 32 is the number of output channels\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # relu layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # average pooling layer \n",
    "        #self.pool = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # q: what is nn.AdaptiveAvgPool1d(1)?\n",
    "        # a: nn.AdaptiveAvgPool1d(1) is a function that averages the input\n",
    "        #self.globalavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        # bactchnormalization before activation\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # q: explain nn.relu(x) vs nn.functional.relu(x)\n",
    "        # a: nn.relu(x) is a module, nn.functional.relu(x) is a function\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        # q: batch normalizing and maxpooling?\n",
    "        # a: batch normalizing is a technique to normalize the input of each layer\n",
    "        #    maxpooling is a technique to reduce the size of the input\n",
    "\n",
    "        # printing the shape of x\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Resnet block of the Network\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, output_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
    "        self.conv2 = nn.Conv1d(output_channels, output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, output_channels, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm1d(output_channels)\n",
    "        )\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        #print(x.shape)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.shape)\n",
    "        if self.stride != 1 or x.shape[1] != out.shape[1]:\n",
    "            residual = self.downsample(x)\n",
    "        #print(residual.shape)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "\n",
    "# The model with the convolutional block\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn = CNNblock(input_channels)\n",
    "        self.fc = nn.Linear(64*5000, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # what is x.view(x.size(0), -1)?\n",
    "        # a: x.view(x.size(0), -1) is a function that flattens the input\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 2500, 128)  # Flattened size after pooling\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Resnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolutional layer of the residual block\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second convolutional layer of the residual block\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # Pass input through the first convolutional layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        # Pass the output of the first convolutional layer through the second convolutional layer\n",
    "        out = self.conv2(out)\n",
    "        # Add the residual connection\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual CNN model\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv1d(8, 16, kernel_size=2, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # First residual block\n",
    "        self.res_block1 = ResidualBlock(16, 16)\n",
    "        # Second residual block\n",
    "        self.res_block2 = ResidualBlock(16, 16) # remove this \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(16 * 2500, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the initial convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # Pass the output through the first residual block\n",
    "        x = self.res_block1(x)\n",
    "        # Pass the output through the second residual block\n",
    "        x = self.res_block2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass the flattened output through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To clear the VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #print(X.shape)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        #print(\"LOSS\")   # this print statement is there to check the warning\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if batch % 100 == 0:\n",
    "         #   loss, current = loss.item(), (batch + 1) * len(X)\n",
    "          #  print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        \n",
    "    #print the average loss of the epoch\n",
    "    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function with tensorbard\n",
    "def trainTB(dataloader, model, loss_fn, optimizer,epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    # get the number of batches\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #print(X.shape)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        # check the shape of pred and y here\n",
    "        if batch == 1:\n",
    "            print(pred.shape)       # this is [8,1]\n",
    "            print(y.shape)          # this is [8]\n",
    "        #print(\"LOSS\")   # this print statement is there to check the warning\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if batch % 100 == 0:\n",
    "        #    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        #    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        #loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "    loss_avg = total_loss/num_batches\n",
    "    print(f\"Epoch [{epoch+1}], Average Loss: {loss_avg:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", loss_avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The step by step CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple1DCNN(\n",
      "  (conv1): Conv1d(8, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=80000, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train the CNN \n",
    "\n",
    "# input size\n",
    "input_size = 8\n",
    "\n",
    "# create the model object\n",
    "modelCNNsimple = Simple1DCNN(8,1)\n",
    "modelCNNsimple.to(device)\n",
    "\n",
    "print(modelCNNsimple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(modelCNNsimple.parameters(), lr=1e-3)  # You can adjust lr and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4889.563477  [  128/10000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 58.853962  [  128/10000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 52.782303  [  128/10000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 53.903473  [  128/10000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 63.146935  [  128/10000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, modelCNNsimple, loss_fn, optimizer)\n",
    "    #test(validate_dataloader, modelCNNsimple, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step reimplimentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create the model object\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m modelCNN \u001b[38;5;241m=\u001b[39m CNN(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m modelCNN\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(modelCNN)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CNN' is not defined"
     ]
    }
   ],
   "source": [
    "# train the CNN \n",
    "\n",
    "# input size\n",
    "input_size = 8\n",
    "\n",
    "# create the model object\n",
    "modelCNN = CNN(8,1)\n",
    "modelCNN.to(device)\n",
    "\n",
    "print(modelCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \u001b[38;5;66;03m# Mean Squared Error loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Adam optimizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(modelCNN\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelCNN' is not defined"
     ]
    }
   ],
   "source": [
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(modelCNN.parameters(), lr=1e-3)  # You can adjust lr and other hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4849.607422  [  128/10000]\n",
      "loss: 665.255493  [  256/10000]\n",
      "loss: 2766.375244  [  384/10000]\n",
      "loss: 1767.146484  [  512/10000]\n",
      "loss: 257.742554  [  640/10000]\n",
      "loss: 385.777283  [  768/10000]\n",
      "loss: 1080.425171  [  896/10000]\n",
      "loss: 1292.255615  [ 1024/10000]\n",
      "loss: 678.771240  [ 1152/10000]\n",
      "loss: 180.054565  [ 1280/10000]\n",
      "loss: 170.081192  [ 1408/10000]\n",
      "loss: 638.198242  [ 1536/10000]\n",
      "loss: 781.975647  [ 1664/10000]\n",
      "loss: 479.890808  [ 1792/10000]\n",
      "loss: 215.536194  [ 1920/10000]\n",
      "loss: 119.576218  [ 2048/10000]\n",
      "loss: 236.538467  [ 2176/10000]\n",
      "loss: 437.979401  [ 2304/10000]\n",
      "loss: 437.651398  [ 2432/10000]\n",
      "loss: 310.671509  [ 2560/10000]\n",
      "loss: 135.758698  [ 2688/10000]\n",
      "loss: 111.186447  [ 2816/10000]\n",
      "loss: 239.832077  [ 2944/10000]\n",
      "loss: 263.199524  [ 3072/10000]\n",
      "loss: 278.564209  [ 3200/10000]\n",
      "loss: 153.170227  [ 3328/10000]\n",
      "loss: 86.712738  [ 3456/10000]\n",
      "loss: 128.976303  [ 3584/10000]\n",
      "loss: 181.366455  [ 3712/10000]\n",
      "loss: 229.840302  [ 3840/10000]\n",
      "loss: 150.572037  [ 3968/10000]\n",
      "loss: 101.597443  [ 4096/10000]\n",
      "loss: 97.016617  [ 4224/10000]\n",
      "loss: 108.971443  [ 4352/10000]\n",
      "loss: 143.898941  [ 4480/10000]\n",
      "loss: 140.417694  [ 4608/10000]\n",
      "loss: 116.039520  [ 4736/10000]\n",
      "loss: 78.603790  [ 4864/10000]\n",
      "loss: 96.506599  [ 4992/10000]\n",
      "loss: 92.129532  [ 5120/10000]\n",
      "loss: 110.489441  [ 5248/10000]\n",
      "loss: 104.572250  [ 5376/10000]\n",
      "loss: 102.219162  [ 5504/10000]\n",
      "loss: 66.504745  [ 5632/10000]\n",
      "loss: 103.796677  [ 5760/10000]\n",
      "loss: 96.530853  [ 5888/10000]\n",
      "loss: 96.500404  [ 6016/10000]\n",
      "loss: 77.061806  [ 6144/10000]\n",
      "loss: 65.051414  [ 6272/10000]\n",
      "loss: 92.023727  [ 6400/10000]\n",
      "loss: 106.470726  [ 6528/10000]\n",
      "loss: 74.360725  [ 6656/10000]\n",
      "loss: 70.785889  [ 6784/10000]\n",
      "loss: 72.006783  [ 6912/10000]\n",
      "loss: 79.794258  [ 7040/10000]\n",
      "loss: 69.484467  [ 7168/10000]\n",
      "loss: 80.917778  [ 7296/10000]\n",
      "loss: 60.788727  [ 7424/10000]\n",
      "loss: 84.012924  [ 7552/10000]\n",
      "loss: 67.372726  [ 7680/10000]\n",
      "loss: 69.512375  [ 7808/10000]\n",
      "loss: 80.006798  [ 7936/10000]\n",
      "loss: 66.155006  [ 8064/10000]\n",
      "loss: 68.186310  [ 8192/10000]\n",
      "loss: 95.257538  [ 8320/10000]\n",
      "loss: 73.773125  [ 8448/10000]\n",
      "loss: 83.947449  [ 8576/10000]\n",
      "loss: 58.108101  [ 8704/10000]\n",
      "loss: 62.712601  [ 8832/10000]\n",
      "loss: 87.003754  [ 8960/10000]\n",
      "loss: 49.882080  [ 9088/10000]\n",
      "loss: 71.010933  [ 9216/10000]\n",
      "loss: 62.271633  [ 9344/10000]\n",
      "loss: 69.867790  [ 9472/10000]\n",
      "loss: 65.132538  [ 9600/10000]\n",
      "loss: 55.739796  [ 9728/10000]\n",
      "loss: 60.450356  [ 9856/10000]\n",
      "loss: 56.656555  [ 9984/10000]\n",
      "loss: 60.501343  [ 1264/10000]\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 48.250198  [  128/10000]\n",
      "loss: 83.936996  [  256/10000]\n",
      "loss: 64.244156  [  384/10000]\n",
      "loss: 53.363228  [  512/10000]\n",
      "loss: 65.440781  [  640/10000]\n",
      "loss: 56.322716  [  768/10000]\n",
      "loss: 56.157646  [  896/10000]\n",
      "loss: 70.731674  [ 1024/10000]\n",
      "loss: 55.367603  [ 1152/10000]\n",
      "loss: 68.248344  [ 1280/10000]\n",
      "loss: 69.364876  [ 1408/10000]\n",
      "loss: 58.776627  [ 1536/10000]\n",
      "loss: 59.668190  [ 1664/10000]\n",
      "loss: 50.147694  [ 1792/10000]\n",
      "loss: 63.816574  [ 1920/10000]\n",
      "loss: 68.932365  [ 2048/10000]\n",
      "loss: 60.950100  [ 2176/10000]\n",
      "loss: 58.609123  [ 2304/10000]\n",
      "loss: 73.335083  [ 2432/10000]\n",
      "loss: 64.686409  [ 2560/10000]\n",
      "loss: 67.402145  [ 2688/10000]\n",
      "loss: 60.515038  [ 2816/10000]\n",
      "loss: 65.849777  [ 2944/10000]\n",
      "loss: 59.836372  [ 3072/10000]\n",
      "loss: 58.021591  [ 3200/10000]\n",
      "loss: 58.721676  [ 3328/10000]\n",
      "loss: 56.964191  [ 3456/10000]\n",
      "loss: 59.605869  [ 3584/10000]\n",
      "loss: 58.203415  [ 3712/10000]\n",
      "loss: 52.594536  [ 3840/10000]\n",
      "loss: 58.179470  [ 3968/10000]\n",
      "loss: 59.045631  [ 4096/10000]\n",
      "loss: 56.215393  [ 4224/10000]\n",
      "loss: 53.497490  [ 4352/10000]\n",
      "loss: 66.577042  [ 4480/10000]\n",
      "loss: 56.234058  [ 4608/10000]\n",
      "loss: 49.664555  [ 4736/10000]\n",
      "loss: 62.273090  [ 4864/10000]\n",
      "loss: 54.891006  [ 4992/10000]\n",
      "loss: 47.972687  [ 5120/10000]\n",
      "loss: 61.757118  [ 5248/10000]\n",
      "loss: 71.261543  [ 5376/10000]\n",
      "loss: 78.652924  [ 5504/10000]\n",
      "loss: 60.868610  [ 5632/10000]\n",
      "loss: 71.419769  [ 5760/10000]\n",
      "loss: 70.086609  [ 5888/10000]\n",
      "loss: 75.480682  [ 6016/10000]\n",
      "loss: 52.294621  [ 6144/10000]\n",
      "loss: 62.785179  [ 6272/10000]\n",
      "loss: 65.347870  [ 6400/10000]\n",
      "loss: 55.285484  [ 6528/10000]\n",
      "loss: 65.550140  [ 6656/10000]\n",
      "loss: 65.810539  [ 6784/10000]\n",
      "loss: 75.962692  [ 6912/10000]\n",
      "loss: 52.182041  [ 7040/10000]\n",
      "loss: 55.600868  [ 7168/10000]\n",
      "loss: 61.511803  [ 7296/10000]\n",
      "loss: 69.119514  [ 7424/10000]\n",
      "loss: 57.557091  [ 7552/10000]\n",
      "loss: 64.041809  [ 7680/10000]\n",
      "loss: 79.645828  [ 7808/10000]\n",
      "loss: 64.462997  [ 7936/10000]\n",
      "loss: 59.722477  [ 8064/10000]\n",
      "loss: 53.723816  [ 8192/10000]\n",
      "loss: 53.966064  [ 8320/10000]\n",
      "loss: 67.149429  [ 8448/10000]\n",
      "loss: 47.900951  [ 8576/10000]\n",
      "loss: 55.133221  [ 8704/10000]\n",
      "loss: 65.937775  [ 8832/10000]\n",
      "loss: 66.692398  [ 8960/10000]\n",
      "loss: 67.415688  [ 9088/10000]\n",
      "loss: 58.028023  [ 9216/10000]\n",
      "loss: 67.614639  [ 9344/10000]\n",
      "loss: 63.112595  [ 9472/10000]\n",
      "loss: 64.832466  [ 9600/10000]\n",
      "loss: 56.493439  [ 9728/10000]\n",
      "loss: 55.972992  [ 9856/10000]\n",
      "loss: 46.412346  [ 9984/10000]\n",
      "loss: 74.083023  [ 1264/10000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 62.172707  [  128/10000]\n",
      "loss: 62.201313  [  256/10000]\n",
      "loss: 59.442810  [  384/10000]\n",
      "loss: 50.170280  [  512/10000]\n",
      "loss: 50.722500  [  640/10000]\n",
      "loss: 54.994999  [  768/10000]\n",
      "loss: 62.119957  [  896/10000]\n",
      "loss: 65.478004  [ 1024/10000]\n",
      "loss: 54.847706  [ 1152/10000]\n",
      "loss: 55.004852  [ 1280/10000]\n",
      "loss: 62.033043  [ 1408/10000]\n",
      "loss: 76.989281  [ 1536/10000]\n",
      "loss: 68.695374  [ 1664/10000]\n",
      "loss: 76.702774  [ 1792/10000]\n",
      "loss: 63.020462  [ 1920/10000]\n",
      "loss: 66.948242  [ 2048/10000]\n",
      "loss: 56.329674  [ 2176/10000]\n",
      "loss: 75.819336  [ 2304/10000]\n",
      "loss: 54.382515  [ 2432/10000]\n",
      "loss: 49.164032  [ 2560/10000]\n",
      "loss: 68.343857  [ 2688/10000]\n",
      "loss: 75.759346  [ 2816/10000]\n",
      "loss: 54.500511  [ 2944/10000]\n",
      "loss: 57.980133  [ 3072/10000]\n",
      "loss: 49.891003  [ 3200/10000]\n",
      "loss: 64.992432  [ 3328/10000]\n",
      "loss: 54.009102  [ 3456/10000]\n",
      "loss: 59.821053  [ 3584/10000]\n",
      "loss: 47.470230  [ 3712/10000]\n",
      "loss: 52.243179  [ 3840/10000]\n",
      "loss: 52.131699  [ 3968/10000]\n",
      "loss: 58.214706  [ 4096/10000]\n",
      "loss: 52.655067  [ 4224/10000]\n",
      "loss: 56.350739  [ 4352/10000]\n",
      "loss: 59.860699  [ 4480/10000]\n",
      "loss: 62.221931  [ 4608/10000]\n",
      "loss: 64.607895  [ 4736/10000]\n",
      "loss: 69.351784  [ 4864/10000]\n",
      "loss: 67.642250  [ 4992/10000]\n",
      "loss: 66.669258  [ 5120/10000]\n",
      "loss: 61.446800  [ 5248/10000]\n",
      "loss: 52.586174  [ 5376/10000]\n",
      "loss: 53.506485  [ 5504/10000]\n",
      "loss: 59.664566  [ 5632/10000]\n",
      "loss: 74.635452  [ 5760/10000]\n",
      "loss: 57.340553  [ 5888/10000]\n",
      "loss: 77.072151  [ 6016/10000]\n",
      "loss: 49.899323  [ 6144/10000]\n",
      "loss: 62.480022  [ 6272/10000]\n",
      "loss: 64.272156  [ 6400/10000]\n",
      "loss: 58.440613  [ 6528/10000]\n",
      "loss: 53.441238  [ 6656/10000]\n",
      "loss: 77.947510  [ 6784/10000]\n",
      "loss: 65.266968  [ 6912/10000]\n",
      "loss: 54.060692  [ 7040/10000]\n",
      "loss: 61.336575  [ 7168/10000]\n",
      "loss: 70.833038  [ 7296/10000]\n",
      "loss: 48.660873  [ 7424/10000]\n",
      "loss: 60.134312  [ 7552/10000]\n",
      "loss: 64.452515  [ 7680/10000]\n",
      "loss: 66.443962  [ 7808/10000]\n",
      "loss: 64.038864  [ 7936/10000]\n",
      "loss: 52.897110  [ 8064/10000]\n",
      "loss: 47.392448  [ 8192/10000]\n",
      "loss: 71.993866  [ 8320/10000]\n",
      "loss: 54.266949  [ 8448/10000]\n",
      "loss: 52.008095  [ 8576/10000]\n",
      "loss: 67.773941  [ 8704/10000]\n",
      "loss: 63.793083  [ 8832/10000]\n",
      "loss: 60.763870  [ 8960/10000]\n",
      "loss: 59.122139  [ 9088/10000]\n",
      "loss: 74.641792  [ 9216/10000]\n",
      "loss: 59.920609  [ 9344/10000]\n",
      "loss: 62.490547  [ 9472/10000]\n",
      "loss: 50.319080  [ 9600/10000]\n",
      "loss: 62.985535  [ 9728/10000]\n",
      "loss: 71.647797  [ 9856/10000]\n",
      "loss: 56.767883  [ 9984/10000]\n",
      "loss: 25.541779  [ 1264/10000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 54.404388  [  128/10000]\n",
      "loss: 56.832413  [  256/10000]\n",
      "loss: 52.287895  [  384/10000]\n",
      "loss: 51.274971  [  512/10000]\n",
      "loss: 53.445618  [  640/10000]\n",
      "loss: 70.537628  [  768/10000]\n",
      "loss: 65.383759  [  896/10000]\n",
      "loss: 47.714188  [ 1024/10000]\n",
      "loss: 56.680275  [ 1152/10000]\n",
      "loss: 73.979721  [ 1280/10000]\n",
      "loss: 79.831497  [ 1408/10000]\n",
      "loss: 62.408981  [ 1536/10000]\n",
      "loss: 63.725067  [ 1664/10000]\n",
      "loss: 54.370644  [ 1792/10000]\n",
      "loss: 45.807400  [ 1920/10000]\n",
      "loss: 53.651485  [ 2048/10000]\n",
      "loss: 53.599560  [ 2176/10000]\n",
      "loss: 70.820496  [ 2304/10000]\n",
      "loss: 46.006248  [ 2432/10000]\n",
      "loss: 53.587376  [ 2560/10000]\n",
      "loss: 59.948853  [ 2688/10000]\n",
      "loss: 58.698696  [ 2816/10000]\n",
      "loss: 46.493439  [ 2944/10000]\n",
      "loss: 79.238853  [ 3072/10000]\n",
      "loss: 56.100590  [ 3200/10000]\n",
      "loss: 66.167000  [ 3328/10000]\n",
      "loss: 64.575211  [ 3456/10000]\n",
      "loss: 48.475876  [ 3584/10000]\n",
      "loss: 77.207733  [ 3712/10000]\n",
      "loss: 52.613373  [ 3840/10000]\n",
      "loss: 58.334320  [ 3968/10000]\n",
      "loss: 74.763451  [ 4096/10000]\n",
      "loss: 61.779060  [ 4224/10000]\n",
      "loss: 50.162289  [ 4352/10000]\n",
      "loss: 88.003189  [ 4480/10000]\n",
      "loss: 53.081120  [ 4608/10000]\n",
      "loss: 63.298595  [ 4736/10000]\n",
      "loss: 68.477737  [ 4864/10000]\n",
      "loss: 89.993210  [ 4992/10000]\n",
      "loss: 61.861946  [ 5120/10000]\n",
      "loss: 74.582214  [ 5248/10000]\n",
      "loss: 73.253082  [ 5376/10000]\n",
      "loss: 73.734001  [ 5504/10000]\n",
      "loss: 78.283524  [ 5632/10000]\n",
      "loss: 54.806000  [ 5760/10000]\n",
      "loss: 65.077850  [ 5888/10000]\n",
      "loss: 54.339592  [ 6016/10000]\n",
      "loss: 76.523880  [ 6144/10000]\n",
      "loss: 52.713882  [ 6272/10000]\n",
      "loss: 69.681671  [ 6400/10000]\n",
      "loss: 63.884850  [ 6528/10000]\n",
      "loss: 63.286949  [ 6656/10000]\n",
      "loss: 103.740005  [ 6784/10000]\n",
      "loss: 67.597023  [ 6912/10000]\n",
      "loss: 73.174271  [ 7040/10000]\n",
      "loss: 63.311817  [ 7168/10000]\n",
      "loss: 65.563797  [ 7296/10000]\n",
      "loss: 69.755737  [ 7424/10000]\n",
      "loss: 67.831390  [ 7552/10000]\n",
      "loss: 61.637775  [ 7680/10000]\n",
      "loss: 69.084518  [ 7808/10000]\n",
      "loss: 72.599701  [ 7936/10000]\n",
      "loss: 73.747375  [ 8064/10000]\n",
      "loss: 63.274040  [ 8192/10000]\n",
      "loss: 54.383675  [ 8320/10000]\n",
      "loss: 72.058479  [ 8448/10000]\n",
      "loss: 65.479553  [ 8576/10000]\n",
      "loss: 54.297783  [ 8704/10000]\n",
      "loss: 59.149773  [ 8832/10000]\n",
      "loss: 61.076885  [ 8960/10000]\n",
      "loss: 41.929546  [ 9088/10000]\n",
      "loss: 61.143379  [ 9216/10000]\n",
      "loss: 54.999657  [ 9344/10000]\n",
      "loss: 56.867443  [ 9472/10000]\n",
      "loss: 41.438324  [ 9600/10000]\n",
      "loss: 59.417919  [ 9728/10000]\n",
      "loss: 44.210072  [ 9856/10000]\n",
      "loss: 57.514771  [ 9984/10000]\n",
      "loss: 64.179710  [ 1264/10000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 53.506592  [  128/10000]\n",
      "loss: 92.809738  [  256/10000]\n",
      "loss: 76.870689  [  384/10000]\n",
      "loss: 44.987854  [  512/10000]\n",
      "loss: 97.440971  [  640/10000]\n",
      "loss: 64.596420  [  768/10000]\n",
      "loss: 68.242912  [  896/10000]\n",
      "loss: 87.963547  [ 1024/10000]\n",
      "loss: 64.722336  [ 1152/10000]\n",
      "loss: 90.628166  [ 1280/10000]\n",
      "loss: 88.172760  [ 1408/10000]\n",
      "loss: 75.726357  [ 1536/10000]\n",
      "loss: 97.964371  [ 1664/10000]\n",
      "loss: 69.690994  [ 1792/10000]\n",
      "loss: 60.261314  [ 1920/10000]\n",
      "loss: 59.878544  [ 2048/10000]\n",
      "loss: 59.927139  [ 2176/10000]\n",
      "loss: 61.229004  [ 2304/10000]\n",
      "loss: 81.773514  [ 2432/10000]\n",
      "loss: 85.918755  [ 2560/10000]\n",
      "loss: 53.572796  [ 2688/10000]\n",
      "loss: 81.355423  [ 2816/10000]\n",
      "loss: 64.208626  [ 2944/10000]\n",
      "loss: 45.745605  [ 3072/10000]\n",
      "loss: 67.168884  [ 3200/10000]\n",
      "loss: 56.729057  [ 3328/10000]\n",
      "loss: 68.193466  [ 3456/10000]\n",
      "loss: 69.253761  [ 3584/10000]\n",
      "loss: 62.219280  [ 3712/10000]\n",
      "loss: 63.844082  [ 3840/10000]\n",
      "loss: 71.660370  [ 3968/10000]\n",
      "loss: 66.441589  [ 4096/10000]\n",
      "loss: 58.834515  [ 4224/10000]\n",
      "loss: 75.819870  [ 4352/10000]\n",
      "loss: 79.111099  [ 4480/10000]\n",
      "loss: 66.644348  [ 4608/10000]\n",
      "loss: 59.519085  [ 4736/10000]\n",
      "loss: 36.332409  [ 4864/10000]\n",
      "loss: 67.604721  [ 4992/10000]\n",
      "loss: 56.026821  [ 5120/10000]\n",
      "loss: 72.017624  [ 5248/10000]\n",
      "loss: 56.596329  [ 5376/10000]\n",
      "loss: 46.797916  [ 5504/10000]\n",
      "loss: 56.730984  [ 5632/10000]\n",
      "loss: 51.213249  [ 5760/10000]\n",
      "loss: 54.608345  [ 5888/10000]\n",
      "loss: 63.921944  [ 6016/10000]\n",
      "loss: 54.536346  [ 6144/10000]\n",
      "loss: 59.724319  [ 6272/10000]\n",
      "loss: 82.349777  [ 6400/10000]\n",
      "loss: 57.931858  [ 6528/10000]\n",
      "loss: 72.873474  [ 6656/10000]\n",
      "loss: 66.681808  [ 6784/10000]\n",
      "loss: 59.144054  [ 6912/10000]\n",
      "loss: 84.613983  [ 7040/10000]\n",
      "loss: 62.620209  [ 7168/10000]\n",
      "loss: 51.920265  [ 7296/10000]\n",
      "loss: 57.672096  [ 7424/10000]\n",
      "loss: 95.908401  [ 7552/10000]\n",
      "loss: 52.094826  [ 7680/10000]\n",
      "loss: 68.189247  [ 7808/10000]\n",
      "loss: 60.749352  [ 7936/10000]\n",
      "loss: 67.314362  [ 8064/10000]\n",
      "loss: 53.435699  [ 8192/10000]\n",
      "loss: 43.678753  [ 8320/10000]\n",
      "loss: 59.698280  [ 8448/10000]\n",
      "loss: 58.721882  [ 8576/10000]\n",
      "loss: 66.919479  [ 8704/10000]\n",
      "loss: 58.093491  [ 8832/10000]\n",
      "loss: 61.843040  [ 8960/10000]\n",
      "loss: 63.356270  [ 9088/10000]\n",
      "loss: 90.928864  [ 9216/10000]\n",
      "loss: 64.243774  [ 9344/10000]\n",
      "loss: 59.416847  [ 9472/10000]\n",
      "loss: 56.016678  [ 9600/10000]\n",
      "loss: 69.034760  [ 9728/10000]\n",
      "loss: 98.720779  [ 9856/10000]\n",
      "loss: 65.594406  [ 9984/10000]\n",
      "loss: 40.424324  [ 1264/10000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, modelCNN, loss_fn, optimizer)\n",
    "    #test(validate_dataloader, modelCNN, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff - correct one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KanResWide_X2(\n",
      "  (init_block): KanResInit(\n",
      "    (conv1): Conv1d(8, 64, kernel_size=(8,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (module_blocks): Sequential(\n",
      "    (0): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (6): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (7): KanResModule(\n",
      "      (conv1): Conv1d(32, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 32, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_shape = (8,5000)  # Modify this according to your input shape\n",
    "# 128 is the batch size, 8 is the number of channels, 5000 is the number of time steps\n",
    "\n",
    "output_size = 1  # Number of output units\n",
    "\n",
    "model = KanResWide_X2(input_shape, output_size)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # You can adjust lr and other hyperparameters\n",
    "\n",
    "# use Nadam optimizer\n",
    "optimizerN = optim.NAdam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device):\n",
    "    #print(pr_max_val)\n",
    "    #print(pr_min_val)\n",
    "    #print(qt_max_val)\n",
    "    #print(qt_min_val)\n",
    "    #print(qrs_max_val)\n",
    "    #print(qrs_min_val)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_losses_epoch = []\n",
    "    #val_real_epoch = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            #print(X.shape)\n",
    "            #print(y.shape)\n",
    "            #exit()\n",
    "            # Compute predictions\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            #print(X)\n",
    "            #print(y)        # y is inf?\n",
    "            #print(pred)\n",
    "            #print(loss)\n",
    "            #exit()\n",
    "\n",
    "            #convert pred to real values\n",
    "            #if (y_parameter == 'hr'):\n",
    "                #val_losses_epoch.append(loss.item())\n",
    "                #val_real_epoch.append(loss.item())\n",
    "            #elif (y_parameter == 'pr'):\n",
    "                #predr = pred * (pr_max_val - pr_min_val) + pr_min_val\n",
    "                #lossr = loss_fn(predr, y)\n",
    "                #val_real_epoch.append(lossr.item())\n",
    "            #elif (y_parameter == 'qt'):\n",
    "                #predr = pred * (qt_max_val - qt_min_val) + qt_min_val\n",
    "                #lossr = loss_fn(predr, y)\n",
    "                #val_real_epoch.append(lossr.item())\n",
    "            #elif (y_parameter == 'qrs'):\n",
    "                #predr = pred * (qrs_max_val - qrs_min_val) + qrs_min_val\n",
    "                #lossr = loss_fn(predr, y)\n",
    "                #val_real_epoch.append(lossr.item())\n",
    "\n",
    "            val_losses_epoch.append(loss.item())\n",
    "    print(f\"Validation Loss: {np.mean(val_losses_epoch):.4f}\")\n",
    "    return np.mean(val_losses_epoch) #, np.mean(val_real_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 14120.471680  [10000/10000]\n",
      "Validation Loss: 16971.2314\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1879.231079  [10000/10000]\n",
      "Validation Loss: 517.1336\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1901.704956  [10000/10000]\n",
      "Validation Loss: 438.1177\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 464.814484  [10000/10000]\n",
      "Validation Loss: 404.2698\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 69.059845  [10000/10000]\n",
      "Validation Loss: 367.2397\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 172.954086  [10000/10000]\n",
      "Validation Loss: 393.0334\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 472.719543  [10000/10000]\n",
      "Validation Loss: 428.0073\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 988.655273  [10000/10000]\n",
      "Validation Loss: 575.9935\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 178.297409  [10000/10000]\n",
      "Validation Loss: 390.9721\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 726.472412  [10000/10000]\n",
      "Validation Loss: 383.0849\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 196.629791  [10000/10000]\n",
      "Validation Loss: 357.4712\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 479.432861  [10000/10000]\n",
      "Validation Loss: 341.2014\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 106.658134  [10000/10000]\n",
      "Validation Loss: 356.6347\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 334.073364  [10000/10000]\n",
      "Validation Loss: 363.6269\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 267.904266  [10000/10000]\n",
      "Validation Loss: 376.3590\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 167.166718  [10000/10000]\n",
      "Validation Loss: 396.4908\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 112.659042  [10000/10000]\n",
      "Validation Loss: 344.8016\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 413.064880  [10000/10000]\n",
      "Validation Loss: 366.0572\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 356.898743  [10000/10000]\n",
      "Validation Loss: 384.6328\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 144.746597  [10000/10000]\n",
      "Validation Loss: 392.4949\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 344.725494  [10000/10000]\n",
      "Validation Loss: 386.3431\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 195.480743  [10000/10000]\n",
      "Validation Loss: 340.1470\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 445.099060  [10000/10000]\n",
      "Validation Loss: 372.0858\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 477.148315  [10000/10000]\n",
      "Validation Loss: 350.2475\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 847.600220  [10000/10000]\n",
      "Validation Loss: 359.8976\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 454.184113  [10000/10000]\n",
      "Validation Loss: 394.7351\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 220.790466  [10000/10000]\n",
      "Validation Loss: 398.2060\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 132.946442  [10000/10000]\n",
      "Validation Loss: 360.8719\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 392.398926  [10000/10000]\n",
      "Validation Loss: 352.4400\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 140.274857  [10000/10000]\n",
      "Validation Loss: 382.2562\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 40.808067  [10000/10000]\n",
      "Validation Loss: 365.4450\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 315.933594  [10000/10000]\n",
      "Validation Loss: 369.7667\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 116.288345  [10000/10000]\n",
      "Validation Loss: 383.3615\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 550.911072  [10000/10000]\n",
      "Validation Loss: 369.0173\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 140.564133  [10000/10000]\n",
      "Validation Loss: 385.7265\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 154.422363  [10000/10000]\n",
      "Validation Loss: 388.7874\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 189.483261  [10000/10000]\n",
      "Validation Loss: 407.9690\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 65.260727  [10000/10000]\n",
      "Validation Loss: 388.5886\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 231.854126  [10000/10000]\n",
      "Validation Loss: 400.0869\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 30.933737  [10000/10000]\n",
      "Validation Loss: 404.7169\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 346.428467  [10000/10000]\n",
      "Validation Loss: 381.3927\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 121.365494  [10000/10000]\n",
      "Validation Loss: 441.7693\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 165.143616  [10000/10000]\n",
      "Validation Loss: 385.4484\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 161.080307  [10000/10000]\n",
      "Validation Loss: 429.6043\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 248.342712  [10000/10000]\n",
      "Validation Loss: 444.5387\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 174.684784  [10000/10000]\n",
      "Validation Loss: 371.5513\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 167.952118  [10000/10000]\n",
      "Validation Loss: 406.1830\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 152.085571  [10000/10000]\n",
      "Validation Loss: 401.9958\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 85.025612  [10000/10000]\n",
      "Validation Loss: 432.5034\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 145.947662  [10000/10000]\n",
      "Validation Loss: 413.8551\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 190.949768  [10000/10000]\n",
      "Validation Loss: 410.1336\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 100.403488  [10000/10000]\n",
      "Validation Loss: 407.8159\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 107.289520  [10000/10000]\n",
      "Validation Loss: 438.5713\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 191.388840  [10000/10000]\n",
      "Validation Loss: 414.9284\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 102.267899  [10000/10000]\n",
      "Validation Loss: 441.8186\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 121.826157  [10000/10000]\n",
      "Validation Loss: 433.1344\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 199.289551  [10000/10000]\n",
      "Validation Loss: 414.0494\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 181.990952  [10000/10000]\n",
      "Validation Loss: 411.7084\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 213.061737  [10000/10000]\n",
      "Validation Loss: 412.5308\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 267.598511  [10000/10000]\n",
      "Validation Loss: 419.4097\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 68.090775  [10000/10000]\n",
      "Validation Loss: 413.3044\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 126.263565  [10000/10000]\n",
      "Validation Loss: 437.5468\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 117.248016  [10000/10000]\n",
      "Validation Loss: 410.5945\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 121.024353  [10000/10000]\n",
      "Validation Loss: 410.1760\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 110.616074  [10000/10000]\n",
      "Validation Loss: 429.9865\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 72.207726  [10000/10000]\n",
      "Validation Loss: 454.4508\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 85.060516  [10000/10000]\n",
      "Validation Loss: 418.2941\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 33.303574  [10000/10000]\n",
      "Validation Loss: 457.6455\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 12.350075  [10000/10000]\n",
      "Validation Loss: 462.1649\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 138.921173  [10000/10000]\n",
      "Validation Loss: 415.2622\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 51.934135  [10000/10000]\n",
      "Validation Loss: 445.8684\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 73.413773  [10000/10000]\n",
      "Validation Loss: 431.0500\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 43.583275  [10000/10000]\n",
      "Validation Loss: 443.0164\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 77.177750  [10000/10000]\n",
      "Validation Loss: 429.1861\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 83.169937  [10000/10000]\n",
      "Validation Loss: 440.6757\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 69.803802  [10000/10000]\n",
      "Validation Loss: 444.6620\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 59.559269  [10000/10000]\n",
      "Validation Loss: 427.5097\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 64.684082  [10000/10000]\n",
      "Validation Loss: 419.7046\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 151.014587  [10000/10000]\n",
      "Validation Loss: 453.2646\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 25.909389  [10000/10000]\n",
      "Validation Loss: 444.6001\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 20.759138  [10000/10000]\n",
      "Validation Loss: 433.6222\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 44.752434  [10000/10000]\n",
      "Validation Loss: 433.7902\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 102.944763  [10000/10000]\n",
      "Validation Loss: 441.8667\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 40.418964  [10000/10000]\n",
      "Validation Loss: 439.8448\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 51.630302  [10000/10000]\n",
      "Validation Loss: 440.0777\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 49.455261  [10000/10000]\n",
      "Validation Loss: 425.3114\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 38.576424  [10000/10000]\n",
      "Validation Loss: 462.1889\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 81.111031  [10000/10000]\n",
      "Validation Loss: 432.2516\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 43.933216  [10000/10000]\n",
      "Validation Loss: 429.7245\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 71.004333  [10000/10000]\n",
      "Validation Loss: 412.3374\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 58.113594  [10000/10000]\n",
      "Validation Loss: 426.5668\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 95.445923  [10000/10000]\n",
      "Validation Loss: 415.2248\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 29.884762  [10000/10000]\n",
      "Validation Loss: 440.5520\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 39.286575  [10000/10000]\n",
      "Validation Loss: 432.5146\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 29.216717  [10000/10000]\n",
      "Validation Loss: 434.7240\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 28.303139  [10000/10000]\n",
      "Validation Loss: 426.9258\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 59.400043  [10000/10000]\n",
      "Validation Loss: 427.7566\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 54.845634  [10000/10000]\n",
      "Validation Loss: 421.4808\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 52.938488  [10000/10000]\n",
      "Validation Loss: 404.0344\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 48.759277  [10000/10000]\n",
      "Validation Loss: 428.1887\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizerN)\n",
    "    #trainTB(train_dataloader, model, loss_fn, optimizerN,t)\n",
    "    #writer.flush()\n",
    "    x = validate(validate_dataloader, model, loss_fn, device)\n",
    "print(\"Done!\")\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualCNN(num_classes)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the wandb configuration and log hyperparameters\n",
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(losses):\n",
    "    error_sum = 0\n",
    "    for loss in losses:\n",
    "        absolute_error = abs(loss - 0)  # Assuming 0 is the target value\n",
    "        error_sum += absolute_error\n",
    "\n",
    "    mean_absolute_error = error_sum / len(losses)\n",
    "    return mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Done!\n",
      "CPU times: user 3h 36min 46s, sys: 5min 22s, total: 3h 42min 9s\n",
      "Wall time: 16min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    epochs.append(epoch)\n",
    "\n",
    "    train_losses_epoch = [] \n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        train_losses_epoch.append(int(loss))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    train_loss = MAE(train_losses_epoch)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses_epoch = []  # List to store validation losses for the current epoch\n",
    "        for batch, (X_val, y_val) in enumerate(validate_dataloader):\n",
    "            #X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val)\n",
    "\n",
    "            val_losses_epoch.append(int(val_loss))\n",
    "\n",
    "        val_loss = MAE(val_losses_epoch)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "wandb.log({\"ResNet: loss [mean absolute error] vs epoch\" : wandb.plot.line_series(\n",
    "                       xs=epochs, \n",
    "                       ys=[train_losses, val_losses],\n",
    "                       keys=[\"training\", \"validation\"],\n",
    "                       title=\"\",\n",
    "                       xname=\"epochs\")})\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-valley-36</strong> at: <a href='https://wandb.ai/comprehensive-ecg-analysis/ECG-analysis-with-Deep-Learning-on-GPU-accelerators/runs/gp1m3qi7' target=\"_blank\">https://wandb.ai/comprehensive-ecg-analysis/ECG-analysis-with-Deep-Learning-on-GPU-accelerators/runs/gp1m3qi7</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230703_233401-gp1m3qi7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finish\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
