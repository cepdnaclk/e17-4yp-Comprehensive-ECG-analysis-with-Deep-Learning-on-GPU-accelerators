{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import  Dataset, DataLoader # wraps an iterable around the dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()                             # /e17-4yp-Comp.../python-scripts-resnet/PTB-XL\n",
    "parent_directory = os.path.dirname(current_directory)       # /e17-4yp-Comp.../python-scripts-resnet\n",
    "\n",
    "features_csv_path = os.path.join(parent_directory,  'data', 'ptb-xl-a-comprehensive-electrocardiographic-feature-dataset-1.0.1', 'features', '12sl_features.csv')   \n",
    "\n",
    "# path of the record500 folder\n",
    "path_record = os.path.join(parent_directory,  'data', 'ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1', 'records500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have your DataFrame loaded as self.df\n",
    "# Define the percentages for train, validation, and test sets\n",
    "train_percentage = 0.7\n",
    "validation_percentage = 0.15\n",
    "test_percentage = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSet_PTB_XL(Dataset):\n",
    "\n",
    "    def __init__(self, parameter='hr', split=\"train\"):\n",
    "\n",
    "        # data loading\n",
    "        # Skip the header row\n",
    "        self.df = pd.read_csv(features_csv_path) \n",
    "\n",
    "        # Create an empty list to store the indices of rows to be removed\n",
    "        rows_to_remove = [] \n",
    "\n",
    "        # Iterate through the rows\n",
    "        for index, row in self.df.iterrows():\n",
    "            file_index = int(self.df['ecg_id'].values[index])\n",
    "            folder_name = str(file_index // 1000).zfill(2)+'000' \n",
    "            file_name = str(file_index).zfill(5)+'_hr.hea'\n",
    "            ecg_record_path = os.path.join(path_record, folder_name, file_name)\n",
    "            #print(ecg_record_path)\n",
    "\n",
    "            # Check if the ecg_record_path exists\n",
    "            if not os.path.exists(ecg_record_path):\n",
    "                rows_to_remove.append(index)\n",
    "\n",
    "        # Remove rows where ecg_record_path does not exist\n",
    "        self.df.drop(rows_to_remove, inplace=True)\n",
    "        # Reset the DataFrame index if needed\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        \n",
    "        # Calculate the number of samples for each set\n",
    "        total_samples = len(self.df)\n",
    "        num_train = int(train_percentage * total_samples)\n",
    "        num_validation = int(validation_percentage * total_samples)\n",
    "        num_test = total_samples - num_train - num_validation\n",
    "\n",
    "        # Create an array of indices to shuffle\n",
    "        indices = np.arange(total_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Split the shuffled indices into train, validation, and test sets\n",
    "        train_indices = indices[:num_train]\n",
    "        validation_indices = indices[num_train:num_train + num_validation]\n",
    "        test_indices = indices[num_train + num_validation:]\n",
    "\n",
    "        # Create DataFrames for each set\n",
    "        train = self.df.iloc[train_indices]\n",
    "        validation = self.df.iloc[validation_indices]\n",
    "        test = self.df.iloc[test_indices]\n",
    "\n",
    "        # Reset the index for each DataFrame\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        validation.reset_index(drop=True, inplace=True)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        if split==\"train\":\n",
    "            self.df = train\n",
    "        if split==\"validate\":\n",
    "            self.df = validation\n",
    "        elif split==\"test\":\n",
    "            self.df = test\n",
    "        \n",
    "        if parameter == 'hr':   # 'hr' should be replaced\n",
    "            self.df = self.df.dropna(subset=['RR_Mean_Global'])\n",
    "            # Avg RR interval\n",
    "            # in milli seconds\n",
    "            RR = torch.tensor(self.df['RR_Mean_Global'].values, dtype=torch.float32) \n",
    "            # calculate HR\n",
    "            self.y = 60 * 1000/RR\n",
    "\n",
    "        elif parameter == 'qrs':\n",
    "            self.df = self.df.dropna(subset=['QRS_Dur_Global']) \n",
    "            self.y = torch.tensor(self.df['QRS_Dur_Global'].values, dtype=torch.float32)\n",
    "\n",
    "        elif parameter == 'qt':\n",
    "            self.df = self.df.dropna(subset=['QT_Int_Global']) \n",
    "            self.y = torch.tensor(self.df['QT_Int_Global'].values, dtype=torch.float32)\n",
    "        \n",
    "        elif parameter == 'pr': \n",
    "            self.df = self.df.dropna(subset=['PR_Int_Global'])\n",
    "            self.y = torch.tensor(self.df['PR_Int_Global'].values, dtype=torch.float32)\n",
    "        \n",
    "        # Size of the dataset\n",
    "        self.samples = self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # file path\n",
    "        file_index = int(self.df['ecg_id'].values[index])\n",
    "        folder_name = str(file_index // 1000).zfill(2)+'000' \n",
    "        file_name = str(file_index).zfill(5)+'_hr'\n",
    "\n",
    "        # ecg_record_path = os.path.join(self.super_parent_directory,  'data', 'ptb-xl', 'records500', folder_name, file_name)\n",
    "        ecg_record_path = os.path.join(path_record , folder_name, file_name)\n",
    "\n",
    "        # Use wfdb.rdsamp to read both the .dat file and .hea header file\n",
    "        ecg_record_data, ecg_record_header = wfdb.rdsamp(ecg_record_path)\n",
    "\n",
    "        ecg_signals = torch.tensor(ecg_record_data) # convert dataframe values to tensor\n",
    "        \n",
    "        ecg_signals = ecg_signals.float()\n",
    "        \n",
    "        # Transposing the ecg signals\n",
    "        ecg_signals = ecg_signals/6 # normalization\n",
    "        ecg_signals = ecg_signals.t() \n",
    "        \n",
    "        qt = self.y[index]\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return ecg_signals, qt\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KanResInit(nn.Module):\n",
    "    def __init__(self, in_channels, filterno_1, filterno_2, filtersize_1, filtersize_2, stride):\n",
    "        #print(in_channels) --> 8\n",
    "        super(KanResInit, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, filterno_1, filtersize_1, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(filterno_1)\n",
    "        self.conv2 = nn.Conv1d(filterno_1, filterno_2, filtersize_2)\n",
    "        self.bn2 = nn.BatchNorm1d(filterno_2)\n",
    "        # initialize a relu layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanResModule(nn.Module):\n",
    "    def __init__(self, in_channels, filterno_1, filterno_2, filtersize_1, filtersize_2, stride):\n",
    "        super(KanResModule, self).__init__()\n",
    "        # have to use same padding to keep the size of the input and output the same\n",
    "        # calculate the padding needed for same\n",
    "        padding = (filtersize_1 - 1) // 2 + (stride - 1)\n",
    "        self.conv1 = nn.Conv1d(in_channels, filterno_1, filtersize_1, stride=stride, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(filterno_1)\n",
    "        self.conv2 = nn.Conv1d(filterno_1, filterno_2, filtersize_2, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(filterno_2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        #print(x.shape)      \n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x + identity\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanResWide_X2(nn.Module):\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(KanResWide_X2, self).__init__()\n",
    "\n",
    "        #print(input_shape[0])\n",
    "        #print(input_shape[1])\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.init_block = KanResInit(input_shape[0], 64, 32, 8, 3, 1)\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "        self.module_blocks = nn.Sequential(\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1),\n",
    "            KanResModule(32, 64, 32, 50, 50, 1)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.init_block(x)\n",
    "        #print(\"init block trained\")\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pool 1 trained\")\n",
    "        #print(x.shape)\n",
    "        x = self.module_blocks(x)\n",
    "        #print(\"module blocks trained\")\n",
    "        x = self.global_avg_pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #q: explain the above line\n",
    "        #a: it flattens the input\n",
    "        x = self.fc(x)\n",
    "        #print(x.shape)\n",
    "        # squeeze the output\n",
    "        x = torch.squeeze(x)\n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    train_losses_epoch = [] \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #print(X.shape)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_losses_epoch.append(loss.item())\n",
    "    \n",
    "    return np.mean(train_losses_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_losses_epoch = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute predictions\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            val_losses_epoch.append(loss.item())\n",
    "\n",
    "    return np.mean(val_losses_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 is the batch size, 8 is the number of channels, 5000 is the number of time steps\n",
    "input_shape = (12, 5000)  # Modify this according to your input shape // change to (12,5000) for ptbxl\n",
    "# Number of output units\n",
    "output_size = 1 \n",
    "# number of epochs\n",
    "number_of_epochs = 50\n",
    "#\n",
    "learning_rate = 0.0005\n",
    "\n",
    "y_parameters = ['pr']#, 'qrs', 'qt', 'hr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/data/ptb-xl-a-comprehensive-electrocardiographic-feature-dataset-1.0.1/features/12sl_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m y_parameter \u001b[39min\u001b[39;00m y_parameters:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# ECG dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m ECGDataSet_PTB_XL(parameter\u001b[39m=\u001b[39my_parameter, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     validate_dataset \u001b[39m=\u001b[39m ECGDataSet_PTB_XL(parameter\u001b[39m=\u001b[39my_parameter, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# data loaders\u001b[39;00m\n",
      "\u001b[1;32m/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, parameter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhr\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# data loading\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Skip the header row\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(features_csv_path) \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Create an empty list to store the indices of rows to be removed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.ce.pdn.ac.lk/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/PTBXLV2/ptbxl_notebook.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     rows_to_remove \u001b[39m=\u001b[39m [] \n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/projects2/e17-4yp-compreh-ecg-analysis/e17-4yp-Comprehensive-ECG-analysis-with-Deep-Learning-on-GPU-accelerators/python-scripts-resnet/data/ptb-xl-a-comprehensive-electrocardiographic-feature-dataset-1.0.1/features/12sl_features.csv'"
     ]
    }
   ],
   "source": [
    "for y_parameter in y_parameters:\n",
    "\n",
    "    # ECG dataset\n",
    "    train_dataset = ECGDataSet_PTB_XL(parameter=y_parameter, split='train')\n",
    "    validate_dataset = ECGDataSet_PTB_XL(parameter=y_parameter, split='validate')\n",
    "\n",
    "    # data loaders\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True, num_workers=20)\n",
    "    validate_dataloader = DataLoader(dataset=validate_dataset, batch_size=16, shuffle=False, num_workers=20)\n",
    "\n",
    "    # model\n",
    "    model = KanResWide_X2(input_shape, output_size)\n",
    "\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=learning_rate)     \n",
    "    # Loss function for linear values (e.g., regression)\n",
    "    loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "    # train and validate\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        # train\n",
    "        train_loss = train(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        #train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        val_loss = validate(validate_dataloader, model, loss_fn, device)\n",
    "        #val_losses.append(val_loss)\n",
    "\n",
    "        print(\"Training Loss: \", train_loss, \"\\t\", \"Validation Loss: \", val_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
