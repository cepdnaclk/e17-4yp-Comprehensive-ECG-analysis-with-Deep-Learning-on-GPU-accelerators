{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset # wraps an iterable around the dataset\n",
    "from torchvision import datasets    # stores the samples and their corresponding labels\n",
    "from torchvision.transforms import transforms  # transformations we can perform on our dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# q: what is the difference between torch.nn.functional and torch.nn\n",
    "# a: https://discuss.pytorch.org/t/what-is-the-difference-between-torch-nn-and-torch-nn-functional/33597/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"cf61e02cee13abdd3d8a232d29df527bd6cc7f89\"\n",
    "\n",
    "# Set the WANDB_NOTEBOOK_NAME environment variable to the name of your notebook (manually)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"DataLoader.ipynb\"\n",
    "\n",
    "# set the WANDB_TEMP environment variable to a directory where we have write permissions\n",
    "os.environ[\"WANDB_TEMP\"] = os.getcwd()\n",
    "os.environ[\"WANDB_DIR\"] = os.getcwd()\n",
    "os.environ[\"WANDB_CONFIG_DIR\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mECG-analysis-with-Deep-Learning-on-GPU-accelerators\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "wandb.init(project='ECG-analysis-with-Deep-Learning-on-GPU-accelerators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training \n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, split='train'):\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        # data loading\n",
    "        current_directory = os.getcwd()\n",
    "        self.parent_directory = os.path.dirname(current_directory)\n",
    "        train_small_path = os.path.join(self.parent_directory, 'data', 'deepfake-ecg-small', str(self.split) + '.csv')\n",
    "        self.df = pd.read_csv(train_small_path)  # Skip the header row\n",
    "        \n",
    "        # Avg RR interval\n",
    "        # in milli seconds\n",
    "        RR = torch.tensor(self.df['avgrrinterval'].values, dtype=torch.float32)\n",
    "        # calculate HR\n",
    "        self.y = 60 * 1000/RR\n",
    "\n",
    "        # Size of the dataset\n",
    "        self.samples = self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # file path\n",
    "        filename= self.df['patid'].values[index]\n",
    "        asc_path = os.path.join(self.parent_directory, 'data', 'deepfake-ecg-small', str(self.split), str(filename) + '.asc')\n",
    "        \n",
    "        ecg_signals = pd.read_csv( asc_path, header=None, sep=\" \") # read into dataframe\n",
    "        ecg_signals = torch.tensor(ecg_signals.values) # convert dataframe values to tensor\n",
    "        \n",
    "        ecg_signals = ecg_signals.float()\n",
    "        \n",
    "        # Transposing the ecg signals\n",
    "        ecg_signals = ecg_signals/6000 # normalization\n",
    "        ecg_signals = ecg_signals.t() \n",
    "        \n",
    "        qt = self.y[index]\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return ecg_signals, qt\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG dataset\n",
    "train_dataset = ECGDataSet(split='train')\n",
    "validate_dataset = ECGDataSet(split='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first data\n",
    "first_data = train_dataset[0]\n",
    "x, y = first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0212, -0.0270, -0.0237,  ..., -0.0148, -0.0065, -0.0155],\n",
       "        [-0.0002,  0.0000, -0.0077,  ..., -0.0030,  0.0037,  0.0008],\n",
       "        [-0.0055, -0.0013, -0.0045,  ...,  0.0073,  0.0118,  0.0137],\n",
       "        ...,\n",
       "        [-0.0153, -0.0143, -0.0145,  ...,  0.0112,  0.0148,  0.0175],\n",
       "        [-0.0102, -0.0112, -0.0117,  ...,  0.0087,  0.0147,  0.0043],\n",
       "        [ 0.0003, -0.0048, -0.0042,  ...,  0.0115,  0.0213,  0.0192]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59.6421)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5000])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Residual Convoluted Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# It allows you to efficiently load and iterate over batches of data during the training or evaluation process.\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=20)\n",
    "validate_dataloader = DataLoader(dataset=validate_dataset, batch_size=128, shuffle=False, num_workers=20)\n",
    "\n",
    "# q: what is num_workers?\n",
    "# A: num_workers (int, optional) â€“ how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8, 5000]) torch.Size([128])\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.dtype, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet of the paper reimplementation with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KanResWide_X(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        super(KanResWide_X, self).__init__()\n",
    "        #q: what does super(KanResWide_X, self) do?\n",
    "        #a: it returns a proxy object that delegates method calls to a parent or sibling class of type.\n",
    "        #q: what does super(KanResWide_X, self).__init__() do?\n",
    "        #a: it calls the __init__ function of the parent class (nn.Module)\n",
    "\n",
    "        #q: is super(KanResWide_X, self).__init__() same to super().__init__()?\n",
    "        #a: yes, but the former is more explicit\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # initial module (before resnet blocks)\n",
    "        self.kanres_init = nn.Sequential(\n",
    "            nn.Conv1d(input_size, 64, kernel_size=8, stride=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 32, kernel_size=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Resnet block\n",
    "        self.kanres_module = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=50, stride=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 32, kernel_size=50, stride=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.functional.add()        # the skip connection in res block\n",
    "            #q: what does nn.Add() do?\n",
    "            #a: it adds the input to the output\n",
    "        )\n",
    "\n",
    "        self.global_average_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dense = nn.Linear(32, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.kanres_init(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.kanres_module(x)\n",
    "        x = self.global_average_pooling(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class KanResInit(nn.Module):\n",
    "    def __init__(self, in_channels, filterno_1, filterno_2, filtersize_1, filtersize_2, stride):\n",
    "        #print(in_channels) --> 8\n",
    "        super(KanResInit, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, filterno_1, filtersize_1, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(filterno_1)\n",
    "        self.conv2 = nn.Conv1d(filterno_1, filterno_2, filtersize_2)\n",
    "        self.bn2 = nn.BatchNorm1d(filterno_2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "class KanResModule(nn.Module):\n",
    "    def __init__(self, in_channels, filterno_1, filterno_2, filtersize_1, filtersize_2, stride):\n",
    "        super(KanResModule, self).__init__()\n",
    "        # have to use same padding to keep the size of the input and output the same\n",
    "        # calculate the padding needed for same\n",
    "        padding = (filtersize_1 - 1) // 2 + (stride - 1)\n",
    "        self.conv1 = nn.Conv1d(in_channels, filterno_1, filtersize_1, stride=stride, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(filterno_1)\n",
    "        self.conv2 = nn.Conv1d(filterno_1, filterno_2, filtersize_2, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(filterno_2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        #print(x.shape)      \n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x + identity\n",
    "        return x\n",
    "\n",
    "class KanResWide_X2(nn.Module):\n",
    "    def __init__(self, input_shape, output_size):\n",
    "        super(KanResWide_X2, self).__init__()\n",
    "\n",
    "        #print(input_shape[0])\n",
    "        #print(input_shape[1])\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.init_block = KanResInit(input_shape[0], 64, 64, 8, 3, 1)\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "        self.module_blocks = nn.Sequential(\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1),\n",
    "            KanResModule(64, 64, 64, 50, 50, 1)\n",
    "        )\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.init_block(x)\n",
    "        print(\"init block trained\")\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        print(\"pool 1 trained\")\n",
    "        #print(x.shape)\n",
    "        x = self.module_blocks(x)\n",
    "        print(\"module blocks trained\")\n",
    "        x = self.global_avg_pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step reimplementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small 1D CNN just to check if the model is working\n",
    "class CNNblock(nn.Module):\n",
    "    def __init__(self,input_channels):\n",
    "        super(CNNblock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # q: explain nn.Conv1d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # a: input_channels is the number of channels in the input data\n",
    "        #    32 is the number of output channels\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        # q: what is 32?\n",
    "        # a: 32 is the number of output channels\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # relu layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # average pooling layer \n",
    "        #self.pool = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # q: what is nn.AdaptiveAvgPool1d(1)?\n",
    "        # a: nn.AdaptiveAvgPool1d(1) is a function that averages the input\n",
    "        #self.globalavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        # bactchnormalization before activation\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # q: explain nn.relu(x) vs nn.functional.relu(x)\n",
    "        # a: nn.relu(x) is a module, nn.functional.relu(x) is a function\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        # q: batch normalizing and maxpooling?\n",
    "        # a: batch normalizing is a technique to normalize the input of each layer\n",
    "        #    maxpooling is a technique to reduce the size of the input\n",
    "\n",
    "        # printing the shape of x\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Resnet block of the Network\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, output_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(output_channels)\n",
    "        self.conv2 = nn.Conv1d(output_channels, output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, output_channels, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm1d(output_channels)\n",
    "        )\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        #print(x.shape)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        #print(out.shape)\n",
    "        if self.stride != 1 or x.shape[1] != out.shape[1]:\n",
    "            residual = self.downsample(x)\n",
    "        #print(residual.shape)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "\n",
    "# The model with the convolutional block\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn = CNNblock(input_channels)\n",
    "        self.fc = nn.Linear(64*5000, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # what is x.view(x.size(0), -1)?\n",
    "        # a: x.view(x.size(0), -1) is a function that flattens the input\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 2500, 128)  # Flattened size after pooling\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Resnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First convolutional layer of the residual block\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second convolutional layer of the residual block\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # Pass input through the first convolutional layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        # Pass the output of the first convolutional layer through the second convolutional layer\n",
    "        out = self.conv2(out)\n",
    "        # Add the residual connection\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual CNN model\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv1d(8, 16, kernel_size=2, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # First residual block\n",
    "        self.res_block1 = ResidualBlock(16, 16)\n",
    "        # Second residual block\n",
    "        self.res_block2 = ResidualBlock(16, 16) # remove this \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(16 * 2500, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the initial convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # Pass the output through the first residual block\n",
    "        x = self.res_block1(x)\n",
    "        # Pass the output through the second residual block\n",
    "        x = self.res_block2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass the flattened output through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To clear the VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #print(X.shape)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #if batch % 100 == 0:\n",
    "         #   loss, current = loss.item(), (batch + 1) * len(X)\n",
    "          #  print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The step by step CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple1DCNN(\n",
      "  (conv1): Conv1d(8, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=80000, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train the CNN \n",
    "\n",
    "# input size\n",
    "input_size = 8\n",
    "\n",
    "# create the model object\n",
    "modelCNNsimple = Simple1DCNN(8,1)\n",
    "modelCNNsimple.to(device)\n",
    "\n",
    "print(modelCNNsimple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(modelCNNsimple.parameters(), lr=1e-3)  # You can adjust lr and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4889.563477  [  128/10000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 58.853962  [  128/10000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 52.782303  [  128/10000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 53.903473  [  128/10000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 63.146935  [  128/10000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, modelCNNsimple, loss_fn, optimizer)\n",
    "    #test(validate_dataloader, modelCNNsimple, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step reimplimentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (cnn): CNNblock(\n",
      "    (conv1): Conv1d(8, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "    (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (globalavgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (fc): Linear(in_features=320000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train the CNN \n",
    "\n",
    "# input size\n",
    "input_size = 8\n",
    "\n",
    "# create the model object\n",
    "modelCNN = CNN(8,1)\n",
    "modelCNN.to(device)\n",
    "\n",
    "print(modelCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(modelCNN.parameters(), lr=1e-3)  # You can adjust lr and other hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4849.607422  [  128/10000]\n",
      "loss: 665.255493  [  256/10000]\n",
      "loss: 2766.375244  [  384/10000]\n",
      "loss: 1767.146484  [  512/10000]\n",
      "loss: 257.742554  [  640/10000]\n",
      "loss: 385.777283  [  768/10000]\n",
      "loss: 1080.425171  [  896/10000]\n",
      "loss: 1292.255615  [ 1024/10000]\n",
      "loss: 678.771240  [ 1152/10000]\n",
      "loss: 180.054565  [ 1280/10000]\n",
      "loss: 170.081192  [ 1408/10000]\n",
      "loss: 638.198242  [ 1536/10000]\n",
      "loss: 781.975647  [ 1664/10000]\n",
      "loss: 479.890808  [ 1792/10000]\n",
      "loss: 215.536194  [ 1920/10000]\n",
      "loss: 119.576218  [ 2048/10000]\n",
      "loss: 236.538467  [ 2176/10000]\n",
      "loss: 437.979401  [ 2304/10000]\n",
      "loss: 437.651398  [ 2432/10000]\n",
      "loss: 310.671509  [ 2560/10000]\n",
      "loss: 135.758698  [ 2688/10000]\n",
      "loss: 111.186447  [ 2816/10000]\n",
      "loss: 239.832077  [ 2944/10000]\n",
      "loss: 263.199524  [ 3072/10000]\n",
      "loss: 278.564209  [ 3200/10000]\n",
      "loss: 153.170227  [ 3328/10000]\n",
      "loss: 86.712738  [ 3456/10000]\n",
      "loss: 128.976303  [ 3584/10000]\n",
      "loss: 181.366455  [ 3712/10000]\n",
      "loss: 229.840302  [ 3840/10000]\n",
      "loss: 150.572037  [ 3968/10000]\n",
      "loss: 101.597443  [ 4096/10000]\n",
      "loss: 97.016617  [ 4224/10000]\n",
      "loss: 108.971443  [ 4352/10000]\n",
      "loss: 143.898941  [ 4480/10000]\n",
      "loss: 140.417694  [ 4608/10000]\n",
      "loss: 116.039520  [ 4736/10000]\n",
      "loss: 78.603790  [ 4864/10000]\n",
      "loss: 96.506599  [ 4992/10000]\n",
      "loss: 92.129532  [ 5120/10000]\n",
      "loss: 110.489441  [ 5248/10000]\n",
      "loss: 104.572250  [ 5376/10000]\n",
      "loss: 102.219162  [ 5504/10000]\n",
      "loss: 66.504745  [ 5632/10000]\n",
      "loss: 103.796677  [ 5760/10000]\n",
      "loss: 96.530853  [ 5888/10000]\n",
      "loss: 96.500404  [ 6016/10000]\n",
      "loss: 77.061806  [ 6144/10000]\n",
      "loss: 65.051414  [ 6272/10000]\n",
      "loss: 92.023727  [ 6400/10000]\n",
      "loss: 106.470726  [ 6528/10000]\n",
      "loss: 74.360725  [ 6656/10000]\n",
      "loss: 70.785889  [ 6784/10000]\n",
      "loss: 72.006783  [ 6912/10000]\n",
      "loss: 79.794258  [ 7040/10000]\n",
      "loss: 69.484467  [ 7168/10000]\n",
      "loss: 80.917778  [ 7296/10000]\n",
      "loss: 60.788727  [ 7424/10000]\n",
      "loss: 84.012924  [ 7552/10000]\n",
      "loss: 67.372726  [ 7680/10000]\n",
      "loss: 69.512375  [ 7808/10000]\n",
      "loss: 80.006798  [ 7936/10000]\n",
      "loss: 66.155006  [ 8064/10000]\n",
      "loss: 68.186310  [ 8192/10000]\n",
      "loss: 95.257538  [ 8320/10000]\n",
      "loss: 73.773125  [ 8448/10000]\n",
      "loss: 83.947449  [ 8576/10000]\n",
      "loss: 58.108101  [ 8704/10000]\n",
      "loss: 62.712601  [ 8832/10000]\n",
      "loss: 87.003754  [ 8960/10000]\n",
      "loss: 49.882080  [ 9088/10000]\n",
      "loss: 71.010933  [ 9216/10000]\n",
      "loss: 62.271633  [ 9344/10000]\n",
      "loss: 69.867790  [ 9472/10000]\n",
      "loss: 65.132538  [ 9600/10000]\n",
      "loss: 55.739796  [ 9728/10000]\n",
      "loss: 60.450356  [ 9856/10000]\n",
      "loss: 56.656555  [ 9984/10000]\n",
      "loss: 60.501343  [ 1264/10000]\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 48.250198  [  128/10000]\n",
      "loss: 83.936996  [  256/10000]\n",
      "loss: 64.244156  [  384/10000]\n",
      "loss: 53.363228  [  512/10000]\n",
      "loss: 65.440781  [  640/10000]\n",
      "loss: 56.322716  [  768/10000]\n",
      "loss: 56.157646  [  896/10000]\n",
      "loss: 70.731674  [ 1024/10000]\n",
      "loss: 55.367603  [ 1152/10000]\n",
      "loss: 68.248344  [ 1280/10000]\n",
      "loss: 69.364876  [ 1408/10000]\n",
      "loss: 58.776627  [ 1536/10000]\n",
      "loss: 59.668190  [ 1664/10000]\n",
      "loss: 50.147694  [ 1792/10000]\n",
      "loss: 63.816574  [ 1920/10000]\n",
      "loss: 68.932365  [ 2048/10000]\n",
      "loss: 60.950100  [ 2176/10000]\n",
      "loss: 58.609123  [ 2304/10000]\n",
      "loss: 73.335083  [ 2432/10000]\n",
      "loss: 64.686409  [ 2560/10000]\n",
      "loss: 67.402145  [ 2688/10000]\n",
      "loss: 60.515038  [ 2816/10000]\n",
      "loss: 65.849777  [ 2944/10000]\n",
      "loss: 59.836372  [ 3072/10000]\n",
      "loss: 58.021591  [ 3200/10000]\n",
      "loss: 58.721676  [ 3328/10000]\n",
      "loss: 56.964191  [ 3456/10000]\n",
      "loss: 59.605869  [ 3584/10000]\n",
      "loss: 58.203415  [ 3712/10000]\n",
      "loss: 52.594536  [ 3840/10000]\n",
      "loss: 58.179470  [ 3968/10000]\n",
      "loss: 59.045631  [ 4096/10000]\n",
      "loss: 56.215393  [ 4224/10000]\n",
      "loss: 53.497490  [ 4352/10000]\n",
      "loss: 66.577042  [ 4480/10000]\n",
      "loss: 56.234058  [ 4608/10000]\n",
      "loss: 49.664555  [ 4736/10000]\n",
      "loss: 62.273090  [ 4864/10000]\n",
      "loss: 54.891006  [ 4992/10000]\n",
      "loss: 47.972687  [ 5120/10000]\n",
      "loss: 61.757118  [ 5248/10000]\n",
      "loss: 71.261543  [ 5376/10000]\n",
      "loss: 78.652924  [ 5504/10000]\n",
      "loss: 60.868610  [ 5632/10000]\n",
      "loss: 71.419769  [ 5760/10000]\n",
      "loss: 70.086609  [ 5888/10000]\n",
      "loss: 75.480682  [ 6016/10000]\n",
      "loss: 52.294621  [ 6144/10000]\n",
      "loss: 62.785179  [ 6272/10000]\n",
      "loss: 65.347870  [ 6400/10000]\n",
      "loss: 55.285484  [ 6528/10000]\n",
      "loss: 65.550140  [ 6656/10000]\n",
      "loss: 65.810539  [ 6784/10000]\n",
      "loss: 75.962692  [ 6912/10000]\n",
      "loss: 52.182041  [ 7040/10000]\n",
      "loss: 55.600868  [ 7168/10000]\n",
      "loss: 61.511803  [ 7296/10000]\n",
      "loss: 69.119514  [ 7424/10000]\n",
      "loss: 57.557091  [ 7552/10000]\n",
      "loss: 64.041809  [ 7680/10000]\n",
      "loss: 79.645828  [ 7808/10000]\n",
      "loss: 64.462997  [ 7936/10000]\n",
      "loss: 59.722477  [ 8064/10000]\n",
      "loss: 53.723816  [ 8192/10000]\n",
      "loss: 53.966064  [ 8320/10000]\n",
      "loss: 67.149429  [ 8448/10000]\n",
      "loss: 47.900951  [ 8576/10000]\n",
      "loss: 55.133221  [ 8704/10000]\n",
      "loss: 65.937775  [ 8832/10000]\n",
      "loss: 66.692398  [ 8960/10000]\n",
      "loss: 67.415688  [ 9088/10000]\n",
      "loss: 58.028023  [ 9216/10000]\n",
      "loss: 67.614639  [ 9344/10000]\n",
      "loss: 63.112595  [ 9472/10000]\n",
      "loss: 64.832466  [ 9600/10000]\n",
      "loss: 56.493439  [ 9728/10000]\n",
      "loss: 55.972992  [ 9856/10000]\n",
      "loss: 46.412346  [ 9984/10000]\n",
      "loss: 74.083023  [ 1264/10000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 62.172707  [  128/10000]\n",
      "loss: 62.201313  [  256/10000]\n",
      "loss: 59.442810  [  384/10000]\n",
      "loss: 50.170280  [  512/10000]\n",
      "loss: 50.722500  [  640/10000]\n",
      "loss: 54.994999  [  768/10000]\n",
      "loss: 62.119957  [  896/10000]\n",
      "loss: 65.478004  [ 1024/10000]\n",
      "loss: 54.847706  [ 1152/10000]\n",
      "loss: 55.004852  [ 1280/10000]\n",
      "loss: 62.033043  [ 1408/10000]\n",
      "loss: 76.989281  [ 1536/10000]\n",
      "loss: 68.695374  [ 1664/10000]\n",
      "loss: 76.702774  [ 1792/10000]\n",
      "loss: 63.020462  [ 1920/10000]\n",
      "loss: 66.948242  [ 2048/10000]\n",
      "loss: 56.329674  [ 2176/10000]\n",
      "loss: 75.819336  [ 2304/10000]\n",
      "loss: 54.382515  [ 2432/10000]\n",
      "loss: 49.164032  [ 2560/10000]\n",
      "loss: 68.343857  [ 2688/10000]\n",
      "loss: 75.759346  [ 2816/10000]\n",
      "loss: 54.500511  [ 2944/10000]\n",
      "loss: 57.980133  [ 3072/10000]\n",
      "loss: 49.891003  [ 3200/10000]\n",
      "loss: 64.992432  [ 3328/10000]\n",
      "loss: 54.009102  [ 3456/10000]\n",
      "loss: 59.821053  [ 3584/10000]\n",
      "loss: 47.470230  [ 3712/10000]\n",
      "loss: 52.243179  [ 3840/10000]\n",
      "loss: 52.131699  [ 3968/10000]\n",
      "loss: 58.214706  [ 4096/10000]\n",
      "loss: 52.655067  [ 4224/10000]\n",
      "loss: 56.350739  [ 4352/10000]\n",
      "loss: 59.860699  [ 4480/10000]\n",
      "loss: 62.221931  [ 4608/10000]\n",
      "loss: 64.607895  [ 4736/10000]\n",
      "loss: 69.351784  [ 4864/10000]\n",
      "loss: 67.642250  [ 4992/10000]\n",
      "loss: 66.669258  [ 5120/10000]\n",
      "loss: 61.446800  [ 5248/10000]\n",
      "loss: 52.586174  [ 5376/10000]\n",
      "loss: 53.506485  [ 5504/10000]\n",
      "loss: 59.664566  [ 5632/10000]\n",
      "loss: 74.635452  [ 5760/10000]\n",
      "loss: 57.340553  [ 5888/10000]\n",
      "loss: 77.072151  [ 6016/10000]\n",
      "loss: 49.899323  [ 6144/10000]\n",
      "loss: 62.480022  [ 6272/10000]\n",
      "loss: 64.272156  [ 6400/10000]\n",
      "loss: 58.440613  [ 6528/10000]\n",
      "loss: 53.441238  [ 6656/10000]\n",
      "loss: 77.947510  [ 6784/10000]\n",
      "loss: 65.266968  [ 6912/10000]\n",
      "loss: 54.060692  [ 7040/10000]\n",
      "loss: 61.336575  [ 7168/10000]\n",
      "loss: 70.833038  [ 7296/10000]\n",
      "loss: 48.660873  [ 7424/10000]\n",
      "loss: 60.134312  [ 7552/10000]\n",
      "loss: 64.452515  [ 7680/10000]\n",
      "loss: 66.443962  [ 7808/10000]\n",
      "loss: 64.038864  [ 7936/10000]\n",
      "loss: 52.897110  [ 8064/10000]\n",
      "loss: 47.392448  [ 8192/10000]\n",
      "loss: 71.993866  [ 8320/10000]\n",
      "loss: 54.266949  [ 8448/10000]\n",
      "loss: 52.008095  [ 8576/10000]\n",
      "loss: 67.773941  [ 8704/10000]\n",
      "loss: 63.793083  [ 8832/10000]\n",
      "loss: 60.763870  [ 8960/10000]\n",
      "loss: 59.122139  [ 9088/10000]\n",
      "loss: 74.641792  [ 9216/10000]\n",
      "loss: 59.920609  [ 9344/10000]\n",
      "loss: 62.490547  [ 9472/10000]\n",
      "loss: 50.319080  [ 9600/10000]\n",
      "loss: 62.985535  [ 9728/10000]\n",
      "loss: 71.647797  [ 9856/10000]\n",
      "loss: 56.767883  [ 9984/10000]\n",
      "loss: 25.541779  [ 1264/10000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 54.404388  [  128/10000]\n",
      "loss: 56.832413  [  256/10000]\n",
      "loss: 52.287895  [  384/10000]\n",
      "loss: 51.274971  [  512/10000]\n",
      "loss: 53.445618  [  640/10000]\n",
      "loss: 70.537628  [  768/10000]\n",
      "loss: 65.383759  [  896/10000]\n",
      "loss: 47.714188  [ 1024/10000]\n",
      "loss: 56.680275  [ 1152/10000]\n",
      "loss: 73.979721  [ 1280/10000]\n",
      "loss: 79.831497  [ 1408/10000]\n",
      "loss: 62.408981  [ 1536/10000]\n",
      "loss: 63.725067  [ 1664/10000]\n",
      "loss: 54.370644  [ 1792/10000]\n",
      "loss: 45.807400  [ 1920/10000]\n",
      "loss: 53.651485  [ 2048/10000]\n",
      "loss: 53.599560  [ 2176/10000]\n",
      "loss: 70.820496  [ 2304/10000]\n",
      "loss: 46.006248  [ 2432/10000]\n",
      "loss: 53.587376  [ 2560/10000]\n",
      "loss: 59.948853  [ 2688/10000]\n",
      "loss: 58.698696  [ 2816/10000]\n",
      "loss: 46.493439  [ 2944/10000]\n",
      "loss: 79.238853  [ 3072/10000]\n",
      "loss: 56.100590  [ 3200/10000]\n",
      "loss: 66.167000  [ 3328/10000]\n",
      "loss: 64.575211  [ 3456/10000]\n",
      "loss: 48.475876  [ 3584/10000]\n",
      "loss: 77.207733  [ 3712/10000]\n",
      "loss: 52.613373  [ 3840/10000]\n",
      "loss: 58.334320  [ 3968/10000]\n",
      "loss: 74.763451  [ 4096/10000]\n",
      "loss: 61.779060  [ 4224/10000]\n",
      "loss: 50.162289  [ 4352/10000]\n",
      "loss: 88.003189  [ 4480/10000]\n",
      "loss: 53.081120  [ 4608/10000]\n",
      "loss: 63.298595  [ 4736/10000]\n",
      "loss: 68.477737  [ 4864/10000]\n",
      "loss: 89.993210  [ 4992/10000]\n",
      "loss: 61.861946  [ 5120/10000]\n",
      "loss: 74.582214  [ 5248/10000]\n",
      "loss: 73.253082  [ 5376/10000]\n",
      "loss: 73.734001  [ 5504/10000]\n",
      "loss: 78.283524  [ 5632/10000]\n",
      "loss: 54.806000  [ 5760/10000]\n",
      "loss: 65.077850  [ 5888/10000]\n",
      "loss: 54.339592  [ 6016/10000]\n",
      "loss: 76.523880  [ 6144/10000]\n",
      "loss: 52.713882  [ 6272/10000]\n",
      "loss: 69.681671  [ 6400/10000]\n",
      "loss: 63.884850  [ 6528/10000]\n",
      "loss: 63.286949  [ 6656/10000]\n",
      "loss: 103.740005  [ 6784/10000]\n",
      "loss: 67.597023  [ 6912/10000]\n",
      "loss: 73.174271  [ 7040/10000]\n",
      "loss: 63.311817  [ 7168/10000]\n",
      "loss: 65.563797  [ 7296/10000]\n",
      "loss: 69.755737  [ 7424/10000]\n",
      "loss: 67.831390  [ 7552/10000]\n",
      "loss: 61.637775  [ 7680/10000]\n",
      "loss: 69.084518  [ 7808/10000]\n",
      "loss: 72.599701  [ 7936/10000]\n",
      "loss: 73.747375  [ 8064/10000]\n",
      "loss: 63.274040  [ 8192/10000]\n",
      "loss: 54.383675  [ 8320/10000]\n",
      "loss: 72.058479  [ 8448/10000]\n",
      "loss: 65.479553  [ 8576/10000]\n",
      "loss: 54.297783  [ 8704/10000]\n",
      "loss: 59.149773  [ 8832/10000]\n",
      "loss: 61.076885  [ 8960/10000]\n",
      "loss: 41.929546  [ 9088/10000]\n",
      "loss: 61.143379  [ 9216/10000]\n",
      "loss: 54.999657  [ 9344/10000]\n",
      "loss: 56.867443  [ 9472/10000]\n",
      "loss: 41.438324  [ 9600/10000]\n",
      "loss: 59.417919  [ 9728/10000]\n",
      "loss: 44.210072  [ 9856/10000]\n",
      "loss: 57.514771  [ 9984/10000]\n",
      "loss: 64.179710  [ 1264/10000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 53.506592  [  128/10000]\n",
      "loss: 92.809738  [  256/10000]\n",
      "loss: 76.870689  [  384/10000]\n",
      "loss: 44.987854  [  512/10000]\n",
      "loss: 97.440971  [  640/10000]\n",
      "loss: 64.596420  [  768/10000]\n",
      "loss: 68.242912  [  896/10000]\n",
      "loss: 87.963547  [ 1024/10000]\n",
      "loss: 64.722336  [ 1152/10000]\n",
      "loss: 90.628166  [ 1280/10000]\n",
      "loss: 88.172760  [ 1408/10000]\n",
      "loss: 75.726357  [ 1536/10000]\n",
      "loss: 97.964371  [ 1664/10000]\n",
      "loss: 69.690994  [ 1792/10000]\n",
      "loss: 60.261314  [ 1920/10000]\n",
      "loss: 59.878544  [ 2048/10000]\n",
      "loss: 59.927139  [ 2176/10000]\n",
      "loss: 61.229004  [ 2304/10000]\n",
      "loss: 81.773514  [ 2432/10000]\n",
      "loss: 85.918755  [ 2560/10000]\n",
      "loss: 53.572796  [ 2688/10000]\n",
      "loss: 81.355423  [ 2816/10000]\n",
      "loss: 64.208626  [ 2944/10000]\n",
      "loss: 45.745605  [ 3072/10000]\n",
      "loss: 67.168884  [ 3200/10000]\n",
      "loss: 56.729057  [ 3328/10000]\n",
      "loss: 68.193466  [ 3456/10000]\n",
      "loss: 69.253761  [ 3584/10000]\n",
      "loss: 62.219280  [ 3712/10000]\n",
      "loss: 63.844082  [ 3840/10000]\n",
      "loss: 71.660370  [ 3968/10000]\n",
      "loss: 66.441589  [ 4096/10000]\n",
      "loss: 58.834515  [ 4224/10000]\n",
      "loss: 75.819870  [ 4352/10000]\n",
      "loss: 79.111099  [ 4480/10000]\n",
      "loss: 66.644348  [ 4608/10000]\n",
      "loss: 59.519085  [ 4736/10000]\n",
      "loss: 36.332409  [ 4864/10000]\n",
      "loss: 67.604721  [ 4992/10000]\n",
      "loss: 56.026821  [ 5120/10000]\n",
      "loss: 72.017624  [ 5248/10000]\n",
      "loss: 56.596329  [ 5376/10000]\n",
      "loss: 46.797916  [ 5504/10000]\n",
      "loss: 56.730984  [ 5632/10000]\n",
      "loss: 51.213249  [ 5760/10000]\n",
      "loss: 54.608345  [ 5888/10000]\n",
      "loss: 63.921944  [ 6016/10000]\n",
      "loss: 54.536346  [ 6144/10000]\n",
      "loss: 59.724319  [ 6272/10000]\n",
      "loss: 82.349777  [ 6400/10000]\n",
      "loss: 57.931858  [ 6528/10000]\n",
      "loss: 72.873474  [ 6656/10000]\n",
      "loss: 66.681808  [ 6784/10000]\n",
      "loss: 59.144054  [ 6912/10000]\n",
      "loss: 84.613983  [ 7040/10000]\n",
      "loss: 62.620209  [ 7168/10000]\n",
      "loss: 51.920265  [ 7296/10000]\n",
      "loss: 57.672096  [ 7424/10000]\n",
      "loss: 95.908401  [ 7552/10000]\n",
      "loss: 52.094826  [ 7680/10000]\n",
      "loss: 68.189247  [ 7808/10000]\n",
      "loss: 60.749352  [ 7936/10000]\n",
      "loss: 67.314362  [ 8064/10000]\n",
      "loss: 53.435699  [ 8192/10000]\n",
      "loss: 43.678753  [ 8320/10000]\n",
      "loss: 59.698280  [ 8448/10000]\n",
      "loss: 58.721882  [ 8576/10000]\n",
      "loss: 66.919479  [ 8704/10000]\n",
      "loss: 58.093491  [ 8832/10000]\n",
      "loss: 61.843040  [ 8960/10000]\n",
      "loss: 63.356270  [ 9088/10000]\n",
      "loss: 90.928864  [ 9216/10000]\n",
      "loss: 64.243774  [ 9344/10000]\n",
      "loss: 59.416847  [ 9472/10000]\n",
      "loss: 56.016678  [ 9600/10000]\n",
      "loss: 69.034760  [ 9728/10000]\n",
      "loss: 98.720779  [ 9856/10000]\n",
      "loss: 65.594406  [ 9984/10000]\n",
      "loss: 40.424324  [ 1264/10000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, modelCNN, loss_fn, optimizer)\n",
    "    #test(validate_dataloader, modelCNN, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_classes = 1  # Number of output classes\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KanResWide_X2(\n",
      "  (init_block): KanResInit(\n",
      "    (conv1): Conv1d(8, 64, kernel_size=(8,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pool): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  (module_blocks): Sequential(\n",
      "    (0): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (7): KanResModule(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(50,), stride=(1,), padding=same)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_shape = (8,5000)  # Modify this according to your input shape\n",
    "# 128 is the batch size, 8 is the number of channels, 5000 is the number of time steps\n",
    "\n",
    "output_size = 1  # Number of output units\n",
    "\n",
    "model = KanResWide_X2(input_shape, output_size)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # You can adjust lr and other hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4631.524414  [  128/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4605.367676  [  256/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4531.926758  [  384/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4426.579102  [  512/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4342.932617  [  640/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4235.226562  [  768/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4168.889160  [  896/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 4016.171387  [ 1024/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3910.640625  [ 1152/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3808.521973  [ 1280/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3678.368164  [ 1408/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3791.112549  [ 1536/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3653.034180  [ 1664/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3654.121582  [ 1792/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3765.792480  [ 1920/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3763.021973  [ 2048/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3674.426758  [ 2176/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3683.541016  [ 2304/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3536.908691  [ 2432/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3368.702637  [ 2560/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3427.341309  [ 2688/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3378.033691  [ 2816/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3389.193848  [ 2944/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3300.207520  [ 3072/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3195.312012  [ 3200/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3405.301758  [ 3328/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3240.203369  [ 3456/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3262.960449  [ 3584/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3185.394531  [ 3712/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3131.419678  [ 3840/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3198.632324  [ 3968/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3253.584473  [ 4096/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3032.765625  [ 4224/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2905.438965  [ 4352/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2953.299072  [ 4480/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3087.330566  [ 4608/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 3000.765625  [ 4736/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2940.631836  [ 4864/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2926.905273  [ 4992/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2839.929199  [ 5120/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2860.693604  [ 5248/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2847.402344  [ 5376/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2984.036377  [ 5504/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2753.946045  [ 5632/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2863.663086  [ 5760/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2706.875977  [ 5888/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2567.496826  [ 6016/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2873.174561  [ 6144/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2678.222168  [ 6272/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2552.796387  [ 6400/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2600.934570  [ 6528/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2565.501221  [ 6656/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2393.990723  [ 6784/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2615.535645  [ 6912/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2581.573242  [ 7040/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2514.662109  [ 7168/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2648.347168  [ 7296/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2511.932617  [ 7424/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2524.265137  [ 7552/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2300.672852  [ 7680/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2404.458008  [ 7808/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2485.752930  [ 7936/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2290.351318  [ 8064/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2303.830322  [ 8192/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2258.511230  [ 8320/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2345.786133  [ 8448/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2236.491943  [ 8576/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2199.401367  [ 8704/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2154.068359  [ 8832/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2214.033691  [ 8960/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1952.760742  [ 9088/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2004.203857  [ 9216/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1968.849487  [ 9344/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 2081.438965  [ 9472/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1993.199707  [ 9600/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1952.748047  [ 9728/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1988.017212  [ 9856/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1845.745361  [ 9984/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1921.474121  [ 1264/10000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1854.432495  [  128/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1805.377563  [  256/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1841.768066  [  384/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1723.579590  [  512/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1768.981445  [  640/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1574.309570  [  768/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1691.445923  [  896/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1758.195557  [ 1024/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1709.288574  [ 1152/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1629.925659  [ 1280/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1576.538818  [ 1408/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1580.301270  [ 1536/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1578.059326  [ 1664/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1621.674927  [ 1792/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1558.267090  [ 1920/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1495.384277  [ 2048/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1552.874756  [ 2176/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1456.415527  [ 2304/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1285.028931  [ 2432/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1453.137695  [ 2560/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1374.480225  [ 2688/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1334.900635  [ 2816/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1271.676758  [ 2944/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1246.429199  [ 3072/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1363.892578  [ 3200/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1292.735107  [ 3328/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1261.823242  [ 3456/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1315.179565  [ 3584/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1194.228271  [ 3712/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1199.433838  [ 3840/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1137.688843  [ 3968/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1024.905640  [ 4096/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1254.958740  [ 4224/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1088.916748  [ 4352/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 997.299500  [ 4480/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1106.283203  [ 4608/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1099.163208  [ 4736/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1036.518677  [ 4864/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1056.682983  [ 4992/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1023.811340  [ 5120/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 1017.084717  [ 5248/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 996.391907  [ 5376/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 972.272339  [ 5504/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 960.679016  [ 5632/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 822.562378  [ 5760/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 904.489685  [ 5888/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 866.223755  [ 6016/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 884.044495  [ 6144/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 758.547729  [ 6272/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 819.977173  [ 6400/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 797.090576  [ 6528/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 840.937073  [ 6656/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 759.182312  [ 6784/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 698.905273  [ 6912/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 778.867676  [ 7040/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 736.694153  [ 7168/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 750.754150  [ 7296/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 712.294861  [ 7424/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 622.304260  [ 7552/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 643.159424  [ 7680/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 645.253723  [ 7808/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 611.210938  [ 7936/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 609.092712  [ 8064/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 731.402954  [ 8192/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 604.262085  [ 8320/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 626.525818  [ 8448/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 593.054077  [ 8576/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 551.200562  [ 8704/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 549.882812  [ 8832/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 505.452301  [ 8960/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 567.073730  [ 9088/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 521.733032  [ 9216/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 483.670441  [ 9344/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 495.884796  [ 9472/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 456.912781  [ 9600/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 483.701660  [ 9728/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 529.352234  [ 9856/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 423.737823  [ 9984/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 422.386169  [ 1264/10000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 424.412781  [  128/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 388.786438  [  256/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 475.619354  [  384/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 371.234741  [  512/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 399.632690  [  640/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 358.497620  [  768/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 385.353180  [  896/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 347.917450  [ 1024/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 353.466492  [ 1152/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 319.319550  [ 1280/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 320.051270  [ 1408/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 318.818878  [ 1536/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 316.009338  [ 1664/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 344.690430  [ 1792/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 309.813324  [ 1920/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 291.136353  [ 2048/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 243.314133  [ 2176/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 254.728699  [ 2304/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 262.047119  [ 2432/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 296.792542  [ 2560/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 254.528717  [ 2688/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 208.252167  [ 2816/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 263.472198  [ 2944/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 257.931091  [ 3072/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 222.960724  [ 3200/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 267.522919  [ 3328/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 288.018005  [ 3456/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 215.856659  [ 3584/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 175.978516  [ 3712/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 181.159332  [ 3840/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 243.321960  [ 3968/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 136.872665  [ 4096/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 150.682297  [ 4224/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 176.357239  [ 4352/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 160.513489  [ 4480/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 188.831879  [ 4608/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 172.864822  [ 4736/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 159.508423  [ 4864/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 177.002075  [ 4992/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 143.859924  [ 5120/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 142.142212  [ 5248/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 149.540344  [ 5376/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 150.381348  [ 5504/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 128.122986  [ 5632/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 155.299286  [ 5760/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 110.667252  [ 5888/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 130.637375  [ 6016/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 139.928360  [ 6144/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 139.096680  [ 6272/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 115.037842  [ 6400/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 112.006851  [ 6528/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 104.301239  [ 6656/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 139.445114  [ 6784/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 110.650833  [ 6912/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 107.764305  [ 7040/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 106.318031  [ 7168/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 126.431343  [ 7296/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 107.198090  [ 7424/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 141.155212  [ 7552/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 82.028687  [ 7680/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 93.407944  [ 7808/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 100.903847  [ 7936/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 77.170135  [ 8064/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 78.073257  [ 8192/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 95.231140  [ 8320/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 76.173729  [ 8448/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 87.977051  [ 8576/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 59.036812  [ 8704/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 69.892273  [ 8832/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 96.670563  [ 8960/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 76.215080  [ 9088/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 78.489883  [ 9216/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 78.361847  [ 9344/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 71.588791  [ 9472/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 87.201965  [ 9600/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 67.543190  [ 9728/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 44.910793  [ 9856/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 84.157494  [ 9984/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 67.900848  [ 1264/10000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.034714  [  128/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.889824  [  256/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 70.633392  [  384/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.069077  [  512/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.407166  [  640/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 84.000870  [  768/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 48.288727  [  896/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 67.483902  [ 1024/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 76.901520  [ 1152/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 72.636276  [ 1280/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 56.368565  [ 1408/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 66.977036  [ 1536/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.564247  [ 1664/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 72.498795  [ 1792/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 72.582924  [ 1920/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 70.071243  [ 2048/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.850632  [ 2176/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.246433  [ 2304/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 63.464279  [ 2432/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 66.704834  [ 2560/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.786652  [ 2688/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.916641  [ 2816/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 59.853065  [ 2944/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.447765  [ 3072/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.117691  [ 3200/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.304184  [ 3328/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 74.040558  [ 3456/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 52.380630  [ 3584/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 79.649574  [ 3712/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.103352  [ 3840/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 68.365463  [ 3968/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.049366  [ 4096/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 80.654236  [ 4224/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.373070  [ 4352/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 68.569420  [ 4480/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.038139  [ 4608/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 56.956673  [ 4736/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.525826  [ 4864/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.654007  [ 4992/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 76.769882  [ 5120/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 53.174927  [ 5248/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 73.767639  [ 5376/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.039627  [ 5504/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 70.651443  [ 5632/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.874084  [ 5760/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.857849  [ 5888/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 41.375942  [ 6016/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 56.131790  [ 6144/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 45.242226  [ 6272/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.530067  [ 6400/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 48.923630  [ 6528/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 55.943672  [ 6656/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 70.269577  [ 6784/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.569305  [ 6912/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 67.313072  [ 7040/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 46.272339  [ 7168/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.305229  [ 7296/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.361702  [ 7424/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.412144  [ 7552/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.085213  [ 7680/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.886147  [ 7808/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.109318  [ 7936/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 48.757137  [ 8064/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 57.934418  [ 8192/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 66.088638  [ 8320/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 63.832874  [ 8448/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 56.200859  [ 8576/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.438652  [ 8704/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.027370  [ 8832/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.247131  [ 8960/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 66.625618  [ 9088/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.037270  [ 9216/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.204529  [ 9344/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 63.990681  [ 9472/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 63.879860  [ 9600/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.128860  [ 9728/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.411579  [ 9856/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.133698  [ 9984/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 119.201614  [ 1264/10000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.863998  [  128/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.541260  [  256/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.997295  [  384/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.843006  [  512/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.175636  [  640/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 52.332626  [  768/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.749069  [  896/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.137138  [ 1024/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 69.452591  [ 1152/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.973381  [ 1280/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.412178  [ 1408/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.411415  [ 1536/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 57.686195  [ 1664/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 48.484085  [ 1792/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.179367  [ 1920/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 59.912514  [ 2048/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.614403  [ 2176/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 68.817841  [ 2304/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.002533  [ 2432/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 59.343102  [ 2560/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 45.698017  [ 2688/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 72.104980  [ 2816/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.997643  [ 2944/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.370602  [ 3072/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 55.943687  [ 3200/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 53.516434  [ 3328/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 56.310555  [ 3456/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 41.248070  [ 3584/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.973763  [ 3712/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.468559  [ 3840/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.337769  [ 3968/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 68.818924  [ 4096/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 66.046570  [ 4224/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 70.331894  [ 4352/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.351616  [ 4480/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.211491  [ 4608/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 46.339378  [ 4736/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.519009  [ 4864/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.754196  [ 4992/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 42.137016  [ 5120/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 68.096352  [ 5248/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 65.133461  [ 5376/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.261452  [ 5504/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.030399  [ 5632/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 57.032791  [ 5760/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.029697  [ 5888/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 59.350815  [ 6016/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.621735  [ 6144/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.323238  [ 6272/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.528145  [ 6400/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.421616  [ 6528/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.061646  [ 6656/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.691010  [ 6784/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.419724  [ 6912/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 55.616585  [ 7040/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 58.871002  [ 7168/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 61.625519  [ 7296/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 48.817940  [ 7424/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 55.824421  [ 7552/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 49.858078  [ 7680/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 64.263809  [ 7808/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.037163  [ 7936/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 60.762234  [ 8064/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 73.689537  [ 8192/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 47.184067  [ 8320/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 52.118633  [ 8448/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 68.275803  [ 8576/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 48.381561  [ 8704/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 53.440948  [ 8832/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 66.210464  [ 8960/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 54.221645  [ 9088/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 57.968113  [ 9216/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 62.903259  [ 9344/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 46.140076  [ 9472/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 50.211033  [ 9600/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 74.337959  [ 9728/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 55.179176  [ 9856/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 51.897758  [ 9984/10000]\n",
      "init block trained\n",
      "pool 1 trained\n",
      "module blocks trained\n",
      "loss: 40.667946  [ 1264/10000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    #test(validate_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualCNN(num_classes)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the wandb configuration and log hyperparameters\n",
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(losses):\n",
    "    error_sum = 0\n",
    "    for loss in losses:\n",
    "        absolute_error = abs(loss - 0)  # Assuming 0 is the target value\n",
    "        error_sum += absolute_error\n",
    "\n",
    "    mean_absolute_error = error_sum / len(losses)\n",
    "    return mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Done!\n",
      "CPU times: user 3h 36min 46s, sys: 5min 22s, total: 3h 42min 9s\n",
      "Wall time: 16min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(wandb.config.num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    epochs.append(epoch)\n",
    "\n",
    "    train_losses_epoch = [] \n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        train_losses_epoch.append(int(loss))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    train_loss = MAE(train_losses_epoch)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses_epoch = []  # List to store validation losses for the current epoch\n",
    "        for batch, (X_val, y_val) in enumerate(validate_dataloader):\n",
    "            #X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val)\n",
    "\n",
    "            val_losses_epoch.append(int(val_loss))\n",
    "\n",
    "        val_loss = MAE(val_losses_epoch)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "wandb.log({\"ResNet: loss [mean absolute error] vs epoch\" : wandb.plot.line_series(\n",
    "                       xs=epochs, \n",
    "                       ys=[train_losses, val_losses],\n",
    "                       keys=[\"training\", \"validation\"],\n",
    "                       title=\"\",\n",
    "                       xname=\"epochs\")})\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-valley-36</strong> at: <a href='https://wandb.ai/comprehensive-ecg-analysis/ECG-analysis-with-Deep-Learning-on-GPU-accelerators/runs/gp1m3qi7' target=\"_blank\">https://wandb.ai/comprehensive-ecg-analysis/ECG-analysis-with-Deep-Learning-on-GPU-accelerators/runs/gp1m3qi7</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230703_233401-gp1m3qi7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finish\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
