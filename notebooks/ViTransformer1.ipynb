{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset # wraps an iterable around the dataset\n",
    "from torchvision import datasets    # stores the samples and their corresponding labels\n",
    "from torchvision.transforms import transforms  # transformations we can perform on our dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training \n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, split='train'):\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        # data loading\n",
    "        current_directory = os.getcwd()\n",
    "        self.parent_directory = os.path.dirname(current_directory)\n",
    "        train_small_path = os.path.join(self.parent_directory, 'data', 'deepfake-ecg-small', str(self.split) + '.csv')\n",
    "        self.df = pd.read_csv(train_small_path)  # Skip the header row\n",
    "        \n",
    "        # Avg RR interval\n",
    "        # in milli seconds\n",
    "        RR = torch.tensor(self.df['avgrrinterval'].values, dtype=torch.float32)\n",
    "        # calculate HR\n",
    "        self.y = 60 * 1000/RR\n",
    "\n",
    "        # Size of the dataset\n",
    "        self.samples = self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # file path\n",
    "        filename= self.df['patid'].values[index]\n",
    "        asc_path = os.path.join(self.parent_directory, 'data', 'deepfake-ecg-small', str(self.split), str(filename) + '.asc')\n",
    "        \n",
    "        ecg_signals = pd.read_csv( asc_path, header=None, sep=\" \") # read into dataframe\n",
    "        ecg_signals = torch.tensor(ecg_signals.values) # convert dataframe values to tensor\n",
    "        \n",
    "        ecg_signals = ecg_signals.float()\n",
    "        \n",
    "        # Transposing the ecg signals\n",
    "        ecg_signals = ecg_signals/6000 # normalization\n",
    "        ecg_signals = ecg_signals.t() \n",
    "        \n",
    "        qt = self.y[index]\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return ecg_signals, qt\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG dataset\n",
    "train_dataset = ECGDataSet(split='train')\n",
    "validate_dataset = ECGDataSet(split='validate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# It allows you to efficiently load and iterate over batches of data during the training or evaluation process.\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, num_workers=20)\n",
    "validate_dataloader = DataLoader(dataset=validate_dataset, batch_size=8, shuffle=False, num_workers=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function with tensorbard\n",
    "def trainVIT(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    #size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    total_loss = 0\n",
    "    # get the number of batches\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        \n",
    "        # check the shape of pred and y here\n",
    "        if batch == 1:\n",
    "            print(pred.shape)       # this is [8,1]\n",
    "            print(y.shape)          # this is [8]\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_avg = total_loss/num_batches\n",
    "    print(f\"Epoch [{epoch+1}], Average Loss: {loss_avg:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", loss_avg, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dummy train function just to check the patch embedding\n",
    "def trainPE(dataloader, model1,model2):\n",
    "    #size = len(dataloader.dataset)\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    #loss = 0\n",
    "\n",
    "    #total_loss = 0\n",
    "    # get the number of batches\n",
    "    #num_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model1(X)\n",
    "        print(\"_________\")\n",
    "        pred2 = model2(pred)\n",
    "        \n",
    "        # check the shape of pred and y here\n",
    "        #if batch == 1:\n",
    "         #   print(pred.shape)       # this is [8,1]\n",
    "          #  print(y.shape)          # this is [8]\n",
    "\n",
    "        #loss = loss_fn(pred, y)\n",
    "\n",
    "        #total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "    #loss_avg = total_loss/num_batches\n",
    "    #print(f\"Epoch [{epoch+1}], Average Loss: {loss_avg:.4f}\")\n",
    "    #writer.add_scalar(\"Loss/train\", loss_avg, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image (ECG in our case) into patches and then embed them.\n",
    "\n",
    "    ECG --> 8,5000\n",
    "\n",
    "    Paramerters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of image (ECG) in pixels (samples).    (This is 1D 5000)\n",
    "\n",
    "    patch_size : int\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels. (This is 8)\n",
    "\n",
    "    embed_dim : int\n",
    "        Embedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding.\n",
    "\n",
    "    \"\"\"\n",
    "    # This class is modified so that it works with 1D data.\n",
    "    def __init__(self, img_size=5000, patch_size=50, in_chans=8, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = img_size\n",
    "        patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size)\n",
    "\n",
    "        # embed_dim is the output channel size of the convolutional layer.\n",
    "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape is `(batch_size, in_chans, img_size)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape is `(batch_size, n_patches, embed_dim)`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.proj(x) # (batch_size, embed_dim, n_patches)\n",
    "        # I dont think flatten is needed for 1D data.\n",
    "        #x = x.flatten(2) # flatten with 1st 2 dims intact\n",
    "        # (batch_size, embed_dim, n_patches) --> (batch_size, n_patches, embed_dim)\n",
    "        x = x.transpose(1,2)    # (batch_size, n_patches, embed_dim)\n",
    "        # print the shape of x\n",
    "        #print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Last dimension of the input tensors (embed_dim).\n",
    "        The input and out dimension of per token features\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    scale : float\n",
    "        Normalizing constant for the dot product.\n",
    "\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "\n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # define the dimentionality of each of the heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        # when we concatonate all the heads we should get the same dim as the input\n",
    "        \n",
    "        # from attention is all you need paper\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        # get an embedding and output q, k, v\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "\n",
    "        # get the concatenated output of all the heads and maps to a new mapping\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape is `(batch_size, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape is `(batch_size, n_patches + 1, dim)`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        # n_patches + 1 --> class token as the first token\n",
    "        qkv = self.qkv(x)   # (batch_size, n_patches + 1, 3 * dim) x is mulit dimensional\n",
    "        qkv = qkv.reshape(n_samples,n_tokens,3,self.n_heads,self.head_dim) # (batch_size, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4)    # (3, batch_size, n_heads, n_patches + 1, head_dim)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2,-1)    # (batch_size, n_heads, head_dim, n_patches + 1)\n",
    "        dp = (q @ k_t) * self.scale # (batch_size, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dp.softmax(dim=-1) # (batch_size, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        weighted_avg = attn @ v # (batch_size, n_heads, n_patches + 1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1,2) # (batch_size, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2) # (batch_size, n_patches + 1, dim)\n",
    "\n",
    "        x = self.proj(weighted_avg) # (batch_size, n_patches + 1, dim)\n",
    "        x = self.proj_drop(x) # (batch_size, n_patches + 1, dim)\n",
    "        #print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "\n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer.\n",
    "\n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "\n",
    "    p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The first linear layer.\n",
    "\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "\n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer.\n",
    "\n",
    "    drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape is `(batch_size, n_patches + 1, in_features)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape is `(batch_size, n_patches + 1, out_features)`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc1(x) # (batch_size, n_patches + 1, hidden_features)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)    # (batch_size, n_patches + 1, out_features)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    dim : int\n",
    "        Number of input channels (embed_dim).\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module relative to `dim`.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    norm1, norm2 : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    \n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,dim,n_heads, mlp_ratio=4.0, qkv_basis=True, p=0, attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_basis,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=hidden_features,\n",
    "            out_features=dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape is `(batch_size, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape is `(batch_size, n_patches + 1, dim)`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision Transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Both height and the width of the image (ECG in our case). (This is 1D 5000)\n",
    "\n",
    "    patch_size : int\n",
    "        Both height and the width of the patch.\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels. (This is 8)\n",
    "\n",
    "    n_classes : int\n",
    "        Number of classes to predict.\n",
    "\n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "\n",
    "    depth : int\n",
    "        Number of blocks.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module relative to `embed_dim`.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed` layer.\n",
    "\n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls_token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "\n",
    "    blocks : nn.ModuleList\n",
    "        List of `Block` modules.\n",
    "\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=5000,\n",
    "            patch_size=50,\n",
    "            in_chans=8,\n",
    "            n_classes=1,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            n_heads=12,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            p=0.,\n",
    "            attn_p=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # zero tensors for the class token and the positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                n_heads=n_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_basis=qkv_bias,\n",
    "                p=p,\n",
    "                attn_p=attn_p\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape is `(batch_size, in_chans, img_size)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits: torch.Tensor\n",
    "            Logits over all the classes - `(batch_size, n_classes)`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x) # (batch_size, n_patches + 1, embed_dim)\n",
    "\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1) # (batch_size, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1) # (batch_size, n_patches + 1, embed_dim)\n",
    "        x = x + self.pos_embed # (batch_size, n_patches + 1, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0] # just the cls token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        x = torch.squeeze(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_tokens,\n",
    "            dim_model,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dropout_p,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        #Layers\n",
    "        self.positional_encoder = PositionalEncoder(\n",
    "            dim_model = dim_model,\n",
    "            dropout_p = dropout_p,\n",
    "            max_len = 5000 )\n",
    "        # change this embedding to the patch embedding\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "\n",
    "    def forward(self,\n",
    "                src,\n",
    "                tgt):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        # we permute to obtain size (sequence length, batch_size, dim_model),\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer(src, tgt)\n",
    "        out = self.out(transformer_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv1d(8, 768, kernel_size=(50,), stride=(50,))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_shape = (8,5000)  # Modify this according to your input shape\n",
    "# 128 is the batch size, 8 is the number of channels, 5000 is the number of time steps\n",
    "\n",
    "output_size = 1  # Number of output units\n",
    "\n",
    "model = VisionTransformer(\n",
    "    img_size=5000,\n",
    "    patch_size=50,\n",
    "    in_chans=8,\n",
    "    n_classes=1,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    n_heads=12,\n",
    "    mlp_ratio=4.,\n",
    "    qkv_bias=True,\n",
    "    p=0.,\n",
    "    attn_p=0.\n",
    ")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function for linear values (e.g., regression)\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# use Nadam optimizer\n",
    "optimizerN = optim.NAdam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [1], Average Loss: 195.1032\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [2], Average Loss: 57.2543\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [3], Average Loss: 55.0929\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [4], Average Loss: 46.8162\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [5], Average Loss: 44.7930\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [6], Average Loss: 34.5139\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [7], Average Loss: 32.1793\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [8], Average Loss: 31.5948\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [9], Average Loss: 28.5218\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "Epoch [10], Average Loss: 27.3718\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #trainPE(train_dataloader, model1,model2)\n",
    "    trainVIT(train_dataloader, model, loss_fn, optimizerN, t)\n",
    "print(\"Done!\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
