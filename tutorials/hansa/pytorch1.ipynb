{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tutorail from https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")\n",
    "# just checking if git push works on mac"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Downloading training data from open datasets\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",        # directory of the dataset\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()       # Specifies the transformation to apply to the data. \n",
    "    # In this case it converts the images to PyTorch tensors.\n",
    ")\n",
    "\n",
    "#Downloading test data from open datasets\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1dbb7d5e510>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64]) torch.float32 torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(X.shape, y.shape, X.dtype, y.dtype)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([64, 1, 28, 28]) --> tensor has a size of 64 along the batch dimension, 1 along the channel dimension, 28 along the height dimension, and 28 along the width dimension. It indicates that you have a batch of 64 images, each with a single channel (grayscale), and a resolution of 28x28 pixels. <br> \n",
    "torch.Size([64]) --> Label tensor size <br>\n",
    "torch.float32 torch.int64 --> dtypes of the tensor<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU or MPS if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training \n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mcurrent_device()\n",
      "File \u001b[1;32mc:\\Users\\Hansa Alahakoon\\anaconda3\\envs\\FYP1\\Lib\\site-packages\\torch\\cuda\\__init__.py:674\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    673\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     _lazy_init()\n\u001b[0;32m    675\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32mc:\\Users\\Hansa Alahakoon\\anaconda3\\envs\\FYP1\\Lib\\site-packages\\torch\\cuda\\__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[0;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_init()\n\u001b[0;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# nn.Module --> base class for all neural network modules\n",
    "class NeuralNetwork(nn.Module):\n",
    "    #network archirecture is defined in the init method\n",
    "    def __init__(self):\n",
    "        super().__init__()      #calls the __init__() method of the nn.Module pearent class \n",
    "        #( to ensure that the necessary setup and initialization from the parent class are performed.)\n",
    "        #This is important because the nn.Module class performs important bookkeeping tasks and sets up the internal state of the module.\n",
    "\n",
    "        #self.x are methods below. \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential( # allows to stack multiple layers in a sequential manner\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "\n",
    "    # method to define the forward pass computation of the model\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)   --> __call__ method is used to call the forward method (IMPORTANT)\n",
    "        #x = self.linear_relu_stack(x) this also can be used\n",
    "        x = self.flatten.forward(x)\n",
    "        logits = self.linear_relu_stack.forward(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the modelâ€™s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)  # get the total number of samples in the dataset\n",
    "    model.train()   #sets the model in training mode (Stets the attribute named Training to True for the model instance) \n",
    "    #Dropout, batch normalization, etc. are used during training.\n",
    "\n",
    "    # iterates over the batches in the dataloader\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # moves the input data to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # compute prediction and loss --> Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward() # compute the gradients of the model's parameters with respect to the loss function's output\n",
    "        optimizer.step()    #Update the models parameters an optimization algorithm\n",
    "        optimizer.zero_grad()   # Sets all the gradients to zero. If the gradients are not cleared they ll be accumilated.\n",
    "\n",
    "        # prints the progress of the training\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299604  [    0/60000]\n",
      "loss: 2.288077  [ 6400/60000]\n",
      "loss: 2.281423  [12800/60000]\n",
      "loss: 2.255315  [19200/60000]\n",
      "loss: 2.237913  [25600/60000]\n",
      "loss: 2.212834  [32000/60000]\n",
      "loss: 2.210213  [38400/60000]\n",
      "loss: 2.179493  [44800/60000]\n",
      "loss: 2.171839  [51200/60000]\n",
      "loss: 2.147477  [57600/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.136153  [    0/60000]\n",
      "loss: 2.091348  [ 6400/60000]\n",
      "loss: 2.091101  [12800/60000]\n",
      "loss: 2.081707  [19200/60000]\n",
      "loss: 2.052356  [25600/60000]\n",
      "loss: 1.952492  [32000/60000]\n",
      "loss: 1.966271  [38400/60000]\n",
      "loss: 1.918857  [44800/60000]\n",
      "loss: 1.907349  [51200/60000]\n",
      "loss: 1.873082  [57600/60000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.840935  [    0/60000]\n",
      "loss: 1.815804  [ 6400/60000]\n",
      "loss: 1.742565  [12800/60000]\n",
      "loss: 1.743534  [19200/60000]\n",
      "loss: 1.702162  [25600/60000]\n",
      "loss: 1.621993  [32000/60000]\n",
      "loss: 1.503525  [38400/60000]\n",
      "loss: 1.539052  [44800/60000]\n",
      "loss: 1.494745  [51200/60000]\n",
      "loss: 1.499397  [57600/60000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.483907  [    0/60000]\n",
      "loss: 1.471850  [ 6400/60000]\n",
      "loss: 1.397686  [12800/60000]\n",
      "loss: 1.404200  [19200/60000]\n",
      "loss: 1.388538  [25600/60000]\n",
      "loss: 1.253505  [32000/60000]\n",
      "loss: 1.141606  [38400/60000]\n",
      "loss: 1.280593  [44800/60000]\n",
      "loss: 1.144344  [51200/60000]\n",
      "loss: 1.199243  [57600/60000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.249603  [    0/60000]\n",
      "loss: 1.075026  [ 6400/60000]\n",
      "loss: 1.133608  [12800/60000]\n",
      "loss: 1.196078  [19200/60000]\n",
      "loss: 1.130661  [25600/60000]\n",
      "loss: 1.238744  [32000/60000]\n",
      "loss: 1.043429  [38400/60000]\n",
      "loss: 1.123809  [44800/60000]\n",
      "loss: 1.058403  [51200/60000]\n",
      "loss: 0.946577  [57600/60000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    #test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
